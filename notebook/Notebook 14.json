{
	"name": "Notebook 14",
	"properties": {
		"folder": {
			"name": "AccessADLSGen2"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "testconf",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "cdc3bf1e-bcfd-4882-a4e4-5fbf2545baf7"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2/resourceGroups/test-synapse1/providers/Microsoft.Synapse/workspaces/test-synapse1-fan/bigDataPools/testconf",
				"name": "testconf",
				"type": "Spark",
				"endpoint": "https://test-synapse1-fan.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/testconf",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# -------------------------------------------------------\r\n",
					"# ローデータとデータマートの突合テスト\r\n",
					"#   ローデータとデータマートについて、下記の突合テストを実施する。\r\n",
					"#    1. 行数\r\n",
					"#    2. 列数\r\n",
					"#    3. データの値\r\n",
					"# -------------------------------------------------------\r\n",
					"\r\n",
					"from notebookutils import mssparkutils\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"from pyspark.sql.utils import AnalysisException\r\n",
					"import argparse\r\n",
					"import configparser\r\n",
					"import logging\r\n",
					"import datetime\r\n",
					"from pyspark.sql.window import Window\r\n",
					"import pyspark.sql.functions as F\r\n",
					"import re\r\n",
					"from decimal import Decimal\r\n",
					"\r\n",
					"# -----------------------------------------------------\r\n",
					"# \r\n",
					"# Sparkセッションの作成\r\n",
					"# \r\n",
					"# -----------------------------------------------------\r\n",
					"spark = (SparkSession\r\n",
					"\t.builder\r\n",
					"\t.appName(\"837App\")\r\n",
					"\t.config(\"spark.network.timeout\", \"600s\")\r\n",
					"\t.config(\"spark.executor.heartbeatInterval\", \"10s\")\r\n",
					"#\t.config(\"spark.kryoserializer.buffer.mb\", '32855481')\r\n",
					"\t.config(\"spark.kryoserializer.buffer.max\", \"128m\")\r\n",
					"\t.getOrCreate());\r\n",
					"\r\n",
					"# -----------------------------------------------------\r\n",
					"# \r\n",
					"# ロガーの作成\r\n",
					"# \r\n",
					"# -----------------------------------------------------\r\n",
					"logger  = logging.getLogger(__name__)\r\n",
					"handler = logging.StreamHandler()\r\n",
					"handler.setLevel(logging.INFO)\r\n",
					"handler.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] %(message)s'))\r\n",
					"logger.setLevel(logging.INFO)\r\n",
					"logger.addHandler(handler)\r\n",
					"logger.propagate = False\r\n",
					"\r\n",
					"def spark_read_csv(path, sep, header=True):\r\n",
					"\t\"\"\"\r\n",
					"\tSparkを使ってADLSGen2上のCSV/TSVファイルを読み込み、\r\n",
					"\tSparkDataFrameにして返す。\r\n",
					"\r\n",
					"\tKeyword arguments:\r\n",
					"\tlogger -- ロガー\r\n",
					"\tpath   -- ダウンロード対象のADLSGen2URL。\r\n",
					"\tsep    -- 区切り文字。tsv=r'\\t',csv=','。\r\n",
					"\theader -- ヘッダー。 (default True)\r\n",
					"\t\"\"\"\r\n",
					"\ttry:\r\n",
					"\t\tlogger.info(\"CSV/TSV形式ファイルデータの取得を開始します。[path:{} sep:{} header:{}]\".format(path, sep, header))\r\n",
					"\t\t# CSV・TSV形式ファイルの取得\r\n",
					"\t\tdf = spark.read\\\r\n",
					"\t\t\t.option(\"multiline\", \"true\")\\\r\n",
					"\t\t\t.csv(\r\n",
					"\t\t\t\tpath, \r\n",
					"\t\t\t\tsep=sep, \r\n",
					"\t\t\t\theader=header)\r\n",
					"\t\tlogger.info(\"CSV/TSV形式ファイルデータデータの取得が完了しました。[データ:{}]\".format(df))\r\n",
					"\t\treturn df\r\n",
					"\texcept AnalysisException as e:\r\n",
					"\t\tlogger.error(\"パスにファイルが存在しません。[パス:{} e:{}]\".format(path, e))\r\n",
					"\t\traise\r\n",
					"\texcept Exception as e:\r\n",
					"\t\tlogger.error(\"データ取得に失敗しました。[パス:{} e:{}]\".format(path, e))\r\n",
					"\t\traise\r\n",
					"\r\n",
					"def spark_read_parquet(path, header=True):\r\n",
					"\t\"\"\"\r\n",
					"\tSparkを使ってADLSGen2上のparquetファイルを読み込み、\r\n",
					"\tSparkDataFrameにして返す。\r\n",
					"\r\n",
					"\tKeyword arguments:\r\n",
					"\tlogger -- ロガー\r\n",
					"\tpath   -- ダウンロード対象のADLSGen2URL。\r\n",
					"\theader -- ヘッダー。 (default True)\r\n",
					"\t\"\"\"\r\n",
					"\ttry:\r\n",
					"\t\tlogger.info(\"parquet形式ファイルデータの取得を開始します。[path:{} header:{}]\".format(path, header))\r\n",
					"\t\t# parquet形式ファイルの取得\r\n",
					"\t\tdf = spark.read\\\r\n",
					"\t\t\t.parquet(\r\n",
					"\t\t\t\tpath, \r\n",
					"\t\t\t\theader=header)\r\n",
					"\t\tlogger.info(\"parquet形式ファイルデータデータの取得が完了しました。[データ:{}]\".format(df))\r\n",
					"\t\treturn df\r\n",
					"\texcept AnalysisException as e:\r\n",
					"\t\tlogger.error(\"パスにファイルが存在しません。[パス:{} e:{}]\".format(path, e))\r\n",
					"\t\traise\r\n",
					"\texcept Exception as e:\r\n",
					"\t\tlogger.error(\"データ取得に失敗しました。[パス:{} e:{}]\".format(path, e))\r\n",
					"\t\traise\r\n",
					"\r\n",
					"# def spark_read_text(path):\r\n",
					"# \t\"\"\"\r\n",
					"# \tSparkを使ってADLSGen2上のテキストファイルを読み込み、\r\n",
					"# \tSparkDataFrameにして返す。\r\n",
					"\r\n",
					"# \tKeyword arguments:\r\n",
					"# \tlogger -- ロガー\r\n",
					"# \tpath   -- ダウンロード対象のADLSGen2URL。\r\n",
					"# \t\"\"\"\r\n",
					"# \ttry:\r\n",
					"# \t\tlogger.info(\"テキスト形式ファイルデータの取得を開始します。[path:{}]\".format(path))\r\n",
					"# \t\t# テキスト形式ファイルの取得\r\n",
					"# \t\tdf = spark.read.text(path)\r\n",
					"# \t\tlogger.info(\"テキスト形式ファイルデータの取得が完了しました。[データ:{}]\".format(df))\r\n",
					"# \t\treturn df\r\n",
					"# \texcept AnalysisException as e:\r\n",
					"# \t\tlogger.error(\"パスにファイルが存在しません。[パス:{} e:{}]\".format(path, e))\r\n",
					"# \t\traise\r\n",
					"# \texcept Exception as e:\r\n",
					"# \t\tlogger.error(\"データ取得に失敗しました。[パス:{} e:{}]\".format(path, e))\r\n",
					"# \t\traise\r\n",
					"\r\n",
					"# def spark_write(df_upload, path, format_type=\"parquet\", mode_type=\"overwrite\", coalesce=1):\r\n",
					"# \t\"\"\"\r\n",
					"# \tSparkを使ってADLSGen2上にSparkDataFrame情報をファイル出力します。\r\n",
					"\r\n",
					"# \tKeyword arguments:\r\n",
					"# \tlogger      -- ロガー\r\n",
					"# \tdf_upload   -- アップロード対象データのSparkDataFrame\r\n",
					"# \tpath        -- ダウンロード対象のADLSGen2URL\r\n",
					"# \tformat_type -- ファイル形式 (default parquet)\r\n",
					"# \tmode_type   -- 書き込みモード (default overwrite)\r\n",
					"# \tcoalesce    -- 出力ファイル数 (default 1)\r\n",
					"# \t\"\"\"\r\n",
					"# \ttry:\r\n",
					"# \t\tlogger.info(\"ファイルをアップロードします。[df_upload:{} path:{} format_type:{} mode_type:{} coalesce:{}]\" \\\r\n",
					"# \t\t\t.format(df_upload, path, format_type, mode_type, coalesce))\r\n",
					"# \t\tdf_upload.coalesce(coalesce).write. \\\r\n",
					"# \t\t\tformat(format_type).mode(mode_type).save(path, header = 'true')\r\n",
					"# \texcept Exception as e:\r\n",
					"# \t\tlogger.error(\"ファイルアップロードに失敗しました。[パス:{} e:{}]\".format(path, e))\r\n",
					"# \t\traise\r\n",
					"\r\n",
					"# def check_date(trade_date):\r\n",
					"# \t\"\"\"\r\n",
					"# \t取引日がyyyyMMdd形式であるか確認する。\r\n",
					"# \t形式不備があれば、エラー終了する。\r\n",
					"\r\n",
					"# \tKeyword arguments:\r\n",
					"# \ttrade_date -- 取引日。\r\n",
					"# \t\"\"\"\r\n",
					"# \ttry:\r\n",
					"# \t\t# yyyyMMdd形式想定でdatetimeにcastする\r\n",
					"# \t\tdatetime.datetime.strptime(trade_date,\"%Y%m%d\")\r\n",
					"# \texcept ValueError:\r\n",
					"# \t\tlogger.error(\"取引日がyyyyMMdd形式ではありません。[取引日:{}]\".format(trade_date))\r\n",
					"# \t\traise\r\n",
					"\r\n",
					"def main():\r\n",
					"\t\"\"\"\r\n",
					"\tデータ変換処理を実行する。\r\n",
					"\t\"\"\"\r\n",
					"\r\n",
					"\t# -----------------------------------------------------\r\n",
					"\t# \r\n",
					"\t# Sparkジョブの引数の取得\r\n",
					"\t# \r\n",
					"\t# -----------------------------------------------------\r\n",
					"\tlogger.info(\"# -----------------------------------------------------\")\r\n",
					"\tlogger.info(\"# \")\r\n",
					"\tlogger.info(\"# Sparkジョブの引数の設定\")\r\n",
					"\tlogger.info(\"# \")\r\n",
					"\tlogger.info(\"# -----------------------------------------------------\")\r\n",
					"\r\n",
					"\t# 引数の取得\r\n",
					"\tproject         = 'MONITORING'\r\n",
					"\ttrade_date      = '20230821'\r\n",
					"\tsection         = 'daily_monitoring'\r\n",
					"\tdatamart_table  = 'OAPC_MANAGEMENT_INFO'\r\n",
					"\r\n",
					"\tinputdata_file_path            = 'wasbs://pythontest@testsynpaseblobstorage.blob.core.windows.net/test/EXCEL.OAPC_MANAGEMENT_INFO.txt'\r\n",
					"    # inputdata_file_path              = 'wasbs://pythontest@testsynpaseblobstorage.blob.core.windows.net/test/EXCEL.OAPC_MANAGEMENT_INFO.txt'\r\n",
					"    # inputdata_file_path              ='wasbs://pythontest@testsynpaseblobstorage.blob.core.windows.net/test/EXCEL.OAPC_MANAGEMENT_INFO.txt'\r\n",
					"\t# outputdata_file_path           = 'abfss://datamart@stragegen2prdje001.dfs.core.windows.net/MONITORING/OAPC_MANAGEMENT_INFO/2023/08/21'\r\n",
					"\tservicelink_name               = 'AzureBlobStorage11'\r\n",
					"    # servicelink_name               = 'AzureBlobStorage11'\r\n",
					"\r\n",
					"\t# -----------------------------------------------------\r\n",
					"\t# \r\n",
					"\t# BLOB向けSASトークンの作成\r\n",
					"\t# \r\n",
					"\t# -----------------------------------------------------\r\n",
					"\tlogger.info(\"# -----------------------------------------------------\")\r\n",
					"\tlogger.info(\"# \")\r\n",
					"\tlogger.info(\"# BLOB向けSASトークンの作成\")\r\n",
					"\tlogger.info(\"# \")\r\n",
					"\tlogger.info(\"# -----------------------------------------------------\")\r\n",
					"\t# 出力先のパスからストレージ名とコンテナ名を抽出して、SASトークンを取得する\r\n",
					"\tblob_account_name = 'testsynpaseblobstorage' # replace with your blob name\r\n",
					"\tblob_container_name = 'pythontest' # replace with your container name\r\n",
					"\tblob_relative_path = 'test/EXCEL.OAPC_MANAGEMENT_INFO.txt' # replace with your relative folder path\r\n",
					"\tlinked_service_name = 'AzureBlobStorage11' # replace with your linked service name\r\n",
					"\tblob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name)\r\n",
					"\t# wasb_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\r\n",
					"    # spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token)\r\n",
					"    # spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token)\r\n",
					"    # print('Remote blob path: ' + wasb_path)\r\n",
					"\r\n",
					"    # spark.conf.set(output_storage_path, blob_sas_token)\r\n",
					"    # blob_account_name = 'testsynpaseblobstorage' # replace with your blob name\r\n",
					"    # blob_container_name = 'pythontest' # replace with your container name\r\n",
					"    # blob_relative_path = 'test/EXCEL.OAPC_MANAGEMENT_INFO.txt' # replace with your relative folder path\r\n",
					"    # linked_service_name = 'AzureBlobStorage11' # replace with your linked service name\r\n",
					"\r\n",
					"    # blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name)\r\n",
					"\r\n",
					"    # Allow SPARK to access from Blob remotely\r\n",
					"    # wasb_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\r\n",
					"    # spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token)\r\n",
					"    # print('Remote blob path: ' + wasb_path)\r\n",
					"\r\n",
					"    # wasb_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\r\n",
					"\r\n",
					"    \r\n",
					"\r\n",
					"\t# -----------------------------------------------------\r\n",
					"\t# 各種ファイルのダウンロード\r\n",
					"\t# -----------------------------------------------------\r\n",
					"\t# ローデータのダウンロード\r\n",
					"    # spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token)\r\n",
					"\tspark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token)\r\n",
					"    \r\n",
					"    # df_input_data = spark_read_csv(inputdata_file_path, ',')\r\n",
					"\r\n",
					"\tdf_input_data = spark_read_csv(inputdata_file_path, ',')\r\n",
					"\twindow = Window.orderBy(F.col('monotonically_increasing_id'))\r\n",
					"\tdf_input_data = df_input_data.withColumn(\"monotonically_increasing_id\", F.monotonically_increasing_id())\r\n",
					"\tdf_input_data = df_input_data.withColumn('id', F.row_number().over(window)).drop(\"monotonically_increasing_id\")\r\n",
					"\tdata_diff_cnt = 0\r\n",
					"\tdata_diff_sum = 0.0\r\n",
					"\tdiff_col_name = \"\"\r\n",
					"\tdiff_col_name_warn = \"\"\r\n",
					"\tdata_diff_result = \"\"\r\n",
					"\r\n",
					"\tif (df_input_data.count() > 0):\r\n",
					"\t\t# logger.info(\"インプットとアウトプットの差分レコード数・合計値の計算を実施します。\")\r\n",
					"\t\t\r\n",
					"\t\t# # インプットとアウトプットの差分レコード数算出\r\n",
					"\t\t# data_diff_cnt = (df_input_data.select(df_output_data.columns).sort(\"id\")).subtract(df_output_data.select(df_output_data.columns).sort(\"id\")).count()\r\n",
					"\r\n",
					"\t\t# インプットとアウトプットの値の差分の合計算出\r\n",
					"\r\n",
					"\t\t# エラー発生箇所\r\n",
					"\t\tdf_input_data.show()\r\n",
					"\t\tdf_input_data.withColumnRenamed(\"Python3.8\",\"Python3.8_output\").show()\r\n",
					"\t\tprint(\"=======001==========\")\r\n",
					"\t\t# print([F.col(c.replace('`', '\"')).alias(c + \"_input\") for c in df_input_data.columns])\r\n",
					"\t\tdf_input_data_join = df_input_data.select([F.col(c).alias(\"`\"+c+\"`_output\") for c in df_input_data.columns])\r\n",
					"\t\t# print(df_input_data_join)\r\n",
					"\t\t# print(\"=======002==========\")\r\n",
					"\t\t# print([ F.col(c).alias(c + \"_output\") for c in df_input_data.columns])\r\n",
					"\t\t# df_input_data_join  = df_input_data.select([ F.col(c).alias(c + \"_output\") for c in df_input_data.columns])\r\n",
					"\t\t# print(\"========003=========\")\r\n",
					"\t\t# columns = [ F.col().alias(c + \"_output\") for c in df_input_data.columns]\r\n",
					"\t\t# df_input_data_join  = df_input_data.select([F.col(c.alias(c + \"_output\")) for c in df_input_data.columns])\r\n",
					"\t\t# print(\"=================\")\r\n",
					"\r\n",
					"\t\t# df_output_data_join = df_output_data.select([F.col(c.replace('`', '')).alias(c + \"_output\") for c in df_output_data.columns])\r\n",
					"\t\t# df_join = df_input_data_join.join(df_output_data_join, df_input_data_join.id_input == df_output_data_join.id_output, \"left\")\r\n",
					"\r\n",
					"\t# \t# オンプレとDMTでのレコードの差分値を計算する\r\n",
					"\t# \tfor row in df_output_data.dtypes:\r\n",
					"\t# \t\tif row[1] == 'string':\r\n",
					"\t# \t\t\t# string型の場合\r\n",
					"\t# \t\t\tcnt = (df_input_data.select(row[0])).subtract(df_output_data.select(row[0])).count()\r\n",
					"\t# \t\t\tdata_diff_sum = data_diff_sum + cnt\r\n",
					"\t# \t\t\tif cnt > 0:\r\n",
					"\t# \t\t\t\t# 誤差0以外はNGとする\r\n",
					"\t# \t\t\t\tdiff_col_name = diff_col_name + \",\" + row[0]\r\n",
					"\t# \t\telse:\r\n",
					"\t# \t\t\tdf_temp = df_join.fillna(0).withColumn(\"diff\", F.col(row[0].replace('`', '') + \"_input\") - F.col(row[0].replace('`', '') + \"_output\"))\r\n",
					"\t# \t\t\tcol_sum = df_temp.select(F.abs(F.col(\"diff\"))).groupBy().sum().collect()[0][0]\r\n",
					"\r\n",
					"\t# \t\t\tif col_sum is None:\r\n",
					"\t# \t\t\t\tcol_sum = float(df_temp.fillna(0).fillna(\"0\").withColumn(row[0].replace('`', '') + \"_input\",F.col(row[0].replace('`', '') + \"_input\").cast('decimal')).withColumn(\"diff\", F.col(row[0].replace('`', '') + \"_input\") - F.col(row[0].replace('`', '') + \"_output\")).select(F.abs(F.col(\"diff\"))).groupBy().sum().collect()[0][0])\r\n",
					"\t\t\t\t\t\r\n",
					"\t# \t\t\tdata_diff_sum = data_diff_sum + col_sum\r\n",
					"\t\t\t\t\r\n",
					"\t# \t\t\tif re.match('decimal\\([0-9]+,[0-9]+\\)', row[1]):\r\n",
					"\t# \t\t\t\t# decimal(m,n)型の場合\t\t\t\t\t\r\n",
					"\t# \t\t\t\tif Decimal(col_sum) >= Decimal(Decimal(\"0.1\")**int(row[1].replace(\"(\",\",\").replace(\")\",\",\").split(',')[2])):\r\n",
					"\t# \t\t\t\t\t# 誤差が1e-(DMT側で保持する小数点以下桁数)以上であればNGとする\r\n",
					"\t# \t\t\t\t\tdiff_col_name = diff_col_name + \",\" + row[0]\r\n",
					"\t# \t\t\t\telif Decimal(col_sum) > Decimal(\"0\"):\r\n",
					"\t# \t\t\t\t\t# 0超1e-(DMT側で保持する小数点以下桁数)未満であればWARNとする\t\t\t\t\t\t\t\r\n",
					"\t# \t\t\t\t\tdiff_col_name_warn = diff_col_name_warn + \",\" + row[0]\t\t\t\t\t\r\n",
					"\t# \t\t\telse:\r\n",
					"\t# \t\t\t\t# decimal(m,n)以外の数値型(decimal(n)も含む)の場合\r\n",
					"\t# \t\t\t\tif col_sum > 0:\r\n",
					"\t# \t\t\t\t\t# 誤差が1以上であればNGとする\r\n",
					"\t# \t\t\t\t\tdiff_col_name = diff_col_name + \",\" + row[0]\r\n",
					"\t\t\t\t\r\n",
					"\t# \tif diff_col_name:\r\n",
					"\t# \t\tdata_diff_result = \"NG\"\r\n",
					"\t# \telif diff_col_name_warn:\r\n",
					"\t# \t\tdata_diff_result = \"WARN\"\r\n",
					"\t# \telse:\r\n",
					"\t# \t\tdata_diff_result = \"OK\"\r\n",
					"\r\n",
					"\t# \ttest_result_list = []\r\n",
					"\t# \ttest_result_dict = {\r\n",
					"\t# \t\t\t\"key\"              \t: project + \"_\" + trade_date + \"_\" + datamart_name,\r\n",
					"\t# \t\t\t\"project\"          \t: project,\r\n",
					"\t# \t\t\t\"trade_date\"       \t: trade_date,\r\n",
					"\t# \t\t\t\"datamart_name\"    \t: datamart_name,\r\n",
					"\t# \t\t\t\"rowcnt_input\"     \t: df_input_data.count(),\r\n",
					"\t# \t\t\t\"rowcnt_output\"    \t: df_output_data.count(),\r\n",
					"\t# \t\t\t\"rowcnt_diff\"      \t: df_input_data.count()-df_output_data.count(),\r\n",
					"\t# \t\t\t\"colcnt_input\"     \t: len(df_input_data.columns),\r\n",
					"\t# \t\t\t\"colcnt_output\"    \t: len(df_output_data.columns),\r\n",
					"\t# \t\t\t\"colcnt_diff\"      \t: len(df_input_data.columns)-len(df_output_data.columns),\r\n",
					"\t# \t\t\t\"data_diff_rowcnt\" \t: data_diff_cnt,\r\n",
					"\t# \t\t\t\"data_diff_sum\"    \t: data_diff_sum,\r\n",
					"\t# \t\t\t\"diff_col_name\"    \t: diff_col_name,\r\n",
					"\t# \t\t\t\"diff_col_name_warn\": diff_col_name_warn,\r\n",
					"\t# \t\t\t\"data_diff_result\"\t: data_diff_result \r\n",
					"\t# \t\t}\r\n",
					"\t# \ttest_result_list.append(test_result_dict)\r\n",
					"\t# \tlogger.info(\"テストを実施しました。[test_result_dict:{}]\".format(test_result_list))\r\n",
					"\r\n",
					"\t# \t# カウントをインクリメント\r\n",
					"\t# \tdatamart_count += 1\r\n",
					"\r\n",
					"\t# \t# 結果の暫定出力\r\n",
					"\t# \tdf_test_result = spark.createDataFrame(test_result_list)\r\n",
					"\t# \tdf_test_result = df_test_result.select([\"key\", \"project\", \"trade_date\", \"datamart_name\", \"rowcnt_input\", \"rowcnt_output\", \"rowcnt_diff\", \"colcnt_input\", \"colcnt_output\", \"colcnt_diff\", \"data_diff_rowcnt\", \"data_diff_sum\", \"diff_col_name\", \"diff_col_name_warn\", \"data_diff_result\"])\r\n",
					"\t# \tdf_test_result.show()\r\n",
					"\t# \tspark_write(\r\n",
					"\t# \t\tdf_test_result, \r\n",
					"\t# \t\trename_path_project_datamart(rename_path_yyyymmdd(test_result_path), datamart_name),\r\n",
					"\t# \t\tformat_type=\"parquet\", \r\n",
					"\t# \t\tmode_type=\"overwrite\")\r\n",
					"\r\n",
					"\t# \t# テスト結果NGの場合はエラー終了する\r\n",
					"\t# \talert_target_sum = df_test_result.select(F.sum(F.col(\"rowcnt_diff\"))).groupBy().sum().collect()[0][0]\r\n",
					"\t# \tif alert_target_sum > 0 or diff_col_name:\r\n",
					"\t# \t\terror_count = error_count + 1\r\n",
					"\t# \t\terror_datamart_list.append({\"project\":project ,\"datamart_name\":datamart_name})\r\n",
					"\t\t\t\r\n",
					"\r\n",
					"\t# # テスト結果NGの場合はエラー終了する\r\n",
					"\t# if error_count > 0:\r\n",
					"\t# \traise Exception(\"テスト結果NGでした。[プロジェクト-データマート:{}\".format(error_datamart_list))\r\n",
					"\r\n",
					"\r\n",
					"try:\r\n",
					"\t# メイン処理\r\n",
					"\tmain()\r\n",
					"except Exception as e:\r\n",
					"\tlogger.error(\"ETL実行中にエラーが発生しました。[e:{}]\".format(e))\r\n",
					"\traise\r\n",
					""
				],
				"execution_count": 20
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}