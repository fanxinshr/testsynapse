{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "ワークスペース名",
			"defaultValue": "test-synapse1-fan"
		},
		"ADLSGen2Test_accountKey": {
			"type": "secureString",
			"metadata": "'ADLSGen2Test' の 'accountKey' のセキュリティで保護された文字列"
		},
		"ADLSGen2Test2_accountKey": {
			"type": "secureString",
			"metadata": "'ADLSGen2Test2' の 'accountKey' のセキュリティで保護された文字列"
		},
		"AzureBlobStorage11_connectionString": {
			"type": "secureString",
			"metadata": "'AzureBlobStorage11' の 'connectionString' のセキュリティで保護された文字列"
		},
		"AzureBlobStorage14_connectionString": {
			"type": "secureString",
			"metadata": "'AzureBlobStorage14' の 'connectionString' のセキュリティで保護された文字列"
		},
		"AzureDataLakeStorage1_accountKey": {
			"type": "secureString",
			"metadata": "'AzureDataLakeStorage1' の 'accountKey' のセキュリティで保護された文字列"
		},
		"AzureDataLakeStorage2_accountKey": {
			"type": "secureString",
			"metadata": "'AzureDataLakeStorage2' の 'accountKey' のセキュリティで保護された文字列"
		},
		"test-synapse1-fan-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "'test-synapse1-fan-WorkspaceDefaultSqlServer' の 'connectionString' のセキュリティで保護された文字列",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:test-synapse1-fan.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"ADLSGen2Test_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://datapolicy.dfs.core.windows.net"
		},
		"ADLSGen2Test2_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://yikeren1020.dfs.core.windows.net/"
		},
		"AzureBlobStorage12_properties_typeProperties_serviceEndpoint": {
			"type": "string",
			"defaultValue": "https://testsynpaseblobstorage.blob.core.windows.net/"
		},
		"AzureBlobStorage13_sasUri": {
			"type": "secureString",
			"metadata": "'AzureBlobStorage13' の 'sasUri' のセキュリティで保護された文字列"
		},
		"AzureDataLakeStorage1_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://testfansynapseadlsgen2.dfs.core.windows.net/"
		},
		"AzureDataLakeStorage2_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://testfansynapseadlsgen2.dfs.core.windows.net/"
		},
		"AzureMLService1_properties_typeProperties_subscriptionId": {
			"type": "string",
			"defaultValue": "cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2"
		},
		"AzureMLService1_properties_typeProperties_resourceGroupName": {
			"type": "string",
			"defaultValue": "testmachinelearning"
		},
		"AzureMLService2_properties_typeProperties_subscriptionId": {
			"type": "string",
			"defaultValue": "cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2"
		},
		"AzureMLService2_properties_typeProperties_resourceGroupName": {
			"type": "string",
			"defaultValue": "testmachinelearning2"
		},
		"test-synapse1-fan-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://testsynapse12.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 1')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "ForEach1",
						"type": "ForEach",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@activity('Spark job definition1').output",
								"type": "Expression"
							},
							"batchCount": 2,
							"activities": [
								{
									"name": "Spark job definition1",
									"type": "SparkJob",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"sparkJob": {
											"referenceName": "Spark job definition 1",
											"type": "SparkJobDefinitionReference"
										},
										"conf": {
											"spark.dynamicAllocation.enabled": null,
											"spark.dynamicAllocation.minExecutors": null,
											"spark.dynamicAllocation.maxExecutors": null
										},
										"numExecutors": null
									}
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": [],
				"lastPublishTime": "2023-05-29T05:26:11Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/sparkJobDefinitions/Spark job definition 1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 2')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "Dataflow1",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"source1": {},
									"sink1": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": [],
				"lastPublishTime": "2023-08-14T01:01:37Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/dataflows/Dataflow1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DelimitedText1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "test123.csv",
						"fileSystem": "0530"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "1",
						"type": "String"
					},
					{
						"name": " 'tanaka'",
						"type": "String"
					},
					{
						"name": " 2",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DelimitedText2')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileSystem": "dest"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ADLSGen2Test')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('ADLSGen2Test_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('ADLSGen2Test_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ADLSGen2Test2')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('ADLSGen2Test2_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('ADLSGen2Test2_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureBlobStorage11')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('AzureBlobStorage11_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureBlobStorage12')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"serviceEndpoint": "[parameters('AzureBlobStorage12_properties_typeProperties_serviceEndpoint')]",
					"accountKind": "StorageV2"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureBlobStorage13')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"sasUri": "[parameters('AzureBlobStorage13_sasUri')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureBlobStorage14')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('AzureBlobStorage14_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage1_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage1_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage2')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage2_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage2_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureMLService1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureMLService",
				"typeProperties": {
					"subscriptionId": "[parameters('AzureMLService1_properties_typeProperties_subscriptionId')]",
					"resourceGroupName": "[parameters('AzureMLService1_properties_typeProperties_resourceGroupName')]",
					"mlWorkspaceName": "testmachinelearning",
					"authentication": "MSI"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureMLService2')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureMLService",
				"typeProperties": {
					"subscriptionId": "[parameters('AzureMLService2_properties_typeProperties_subscriptionId')]",
					"resourceGroupName": "[parameters('AzureMLService2_properties_typeProperties_resourceGroupName')]",
					"mlWorkspaceName": "testmachinelearning2",
					"authentication": "MSI"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/test-synapse1-fan-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('test-synapse1-fan-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/test-synapse1-fan-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('test-synapse1-fan-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Dataflow1')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "DelimitedText1",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DelimitedText2",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(output(",
						"          {1} as string,",
						"          { 'tanaka'} as string,",
						"          { 2} as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> sink1"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/DelimitedText1')]",
				"[concat(variables('workspaceId'), '/datasets/DelimitedText2')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE USER \"fanxin@microsoft.com\" FROM EXTERNAL PROVIDER; \n\nCREATE SCHEMA testschema;\n\n\n\n\nCREATE MASTER KEY ENCRYPTION BY PASSWORD = 'Pa$$w0rd1234' ;\n\n-- Create a database scoped credential with Azure storage account key as the secret.\nCREATE DATABASE SCOPED CREDENTIAL AzureStorageCredential\nWITH\n  IDENTITY = 'fanxin' ,\n  SECRET = 'Pa$$w0rd1234' ;\n\n\n\n-- testsynapse12.dfs.core.windows.net\n\nCREATE EXTERNAL DATA SOURCE MyAzureStorage\nWITH\n  ( LOCATION = 'wasbs://test@testsynapse12.dfs.core.windows.net' ,\n    CREDENTIAL = AzureStorageCredential ,\n    TYPE = HADOOP\n  ) ;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SynapseDedicatedSQLPool",
						"poolName": "SynapseDedicatedSQLPool"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP (100) [PersonID]\n,[LastName]\n,[FirstName]\n,[Address]\n,[City]\n FROM [dbo].[Persons]\n\n\n CREATE USER [fanxin@microsoft.com] FROM EXTERNAL PROVIDER;\n\n\n CREATE SCHEMA [dbo];\n\n CREATE USER [fanxin@microsoft.com] FROM EXTERNAL PROVIDER;\n\n\n--synapse sql dedicated pool\n--read \nEXEC sp_addrolemember 'db_exporter', [fanxin@microsoft.com];\n \n\n--write\n--Make sure your user has the permissions to CREATE tables in the [dbo] schema\nGRANT CREATE TABLE TO [fanxin@microsoft.com];\nGRANT ALTER ON SCHEMA::dbo TO [fanxin@microsoft.com];\n--Make sure your user has ADMINISTER DATABASE BULK OPERATIONS permissions\nGRANT ADMINISTER DATABASE BULK OPERATIONS TO [fanxin@microsoft.com];\n--Make sure your user has INSERT permissions on the target table\nGRANT INSERT ON Persons TO [fanxin@microsoft.com]\n \n----to dbo\nGRANT CREATE TABLE TO [fanxin@microsoft.com];\nGRANT ALTER ON SCHEMA::[dbo] TO [fanxin@microsoft.com];\n--Make sure your user has ADMINISTER DATABASE BULK OPERATIONS permissions\nGRANT ADMINISTER DATABASE BULK OPERATIONS TO [fanxin@microsoft.com];\n \n--Make sure your user has INSERT permissions on the target table\nGRANT INSERT ON Persons TO [fanxin@microsoft.com]\n\n\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "SynapseDedicatedSQLPool",
						"poolName": "SynapseDedicatedSQLPool"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 3')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP (100) [PersonID]\n,[LastName]\n,[FirstName]\n,[Address]\n,[City]\n FROM [dbo].[Persons]\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "test",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 5')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP (100) [name]\n,[object_id]\n,[principal_id]\n,[schema_id]\n,[parent_object_id]\n,[type]\n,[type_desc]\n,[create_date]\n,[modify_date]\n,[is_ms_shipped]\n,[is_published]\n,[is_schema_published]\n FROM [sys].[all_objects]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "test",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 6')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP (100) [name]\n,[database_id]\n,[source_database_id]\n,[owner_sid]\n,[create_date]\n,[compatibility_level]\n,[is_stale_page_detection_on]\n,[is_memory_optimized_enabled]\n,[is_data_retention_enabled]\n,[is_ledger_on]\n,[is_change_feed_enabled]\n,[catalog_collation_type]\n,[catalog_collation_type_desc]\n,[physical_database_name]\n,[is_result_set_caching_on]\n,[is_accelerated_database_recovery_on]\n,[is_tempdb_spill_to_remote_store]\n,[delayed_durability_desc]\n,[is_memory_optimized_elevate_to_snapshot_on]\n,[is_federation_member]\n,[is_remote_data_archive_enabled]\n,[is_mixed_page_allocation_on]\n,[is_temporal_history_retention_enabled]\n,[is_transform_noise_words_on]\n,[two_digit_year_cutoff]\n,[containment]\n,[containment_desc]\n,[target_recovery_time_in_seconds]\n,[delayed_durability]\n,[resource_pool_id]\n,[default_language_lcid]\n,[default_language_name]\n,[default_fulltext_language_lcid]\n,[default_fulltext_language_name]\n,[is_nested_triggers_on]\n,[is_date_correlation_on]\n,[is_cdc_enabled]\n,[is_encrypted]\n,[is_honor_broker_priority_on]\n,[replica_id]\n,[group_database_id]\n,[is_distributor]\n,[is_sync_with_backup]\n,[service_broker_guid]\n,[is_broker_enabled]\n,[log_reuse_wait]\n,[log_reuse_wait_desc]\n,[is_parameterization_forced]\n,[is_master_key_encrypted_by_server]\n,[is_query_store_on]\n,[is_published]\n,[is_subscribed]\n,[is_merge_published]\n,[is_recursive_triggers_on]\n,[is_cursor_close_on_commit_on]\n,[is_local_cursor_default]\n,[is_fulltext_enabled]\n,[is_trustworthy_on]\n,[is_db_chaining_on]\n,[is_ansi_padding_on]\n,[is_ansi_warnings_on]\n,[is_arithabort_on]\n,[is_concat_null_yields_null_on]\n,[is_numeric_roundabort_on]\n,[is_quoted_identifier_on]\n,[is_auto_create_stats_on]\n,[is_auto_create_stats_incremental_on]\n,[is_auto_update_stats_on]\n,[is_auto_update_stats_async_on]\n,[is_ansi_null_default_on]\n,[is_ansi_nulls_on]\n,[snapshot_isolation_state_desc]\n,[is_read_committed_snapshot_on]\n,[recovery_model]\n,[recovery_model_desc]\n,[page_verify_option]\n,[page_verify_option_desc]\n,[state]\n,[state_desc]\n,[is_in_standby]\n,[is_cleanly_shutdown]\n,[is_supplemental_logging_enabled]\n,[snapshot_isolation_state]\n,[collation_name]\n,[user_access]\n,[user_access_desc]\n,[is_read_only]\n,[is_auto_close_on]\n,[is_auto_shrink_on]\n FROM [sys].[databases]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "test",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SelectTop100')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP (100) [CONSTRAINT_CATALOG]\n,[CONSTRAINT_SCHEMA]\n,[CONSTRAINT_NAME]\n,[CHECK_CLAUSE]\n FROM [INFORMATION_SCHEMA].[CHECK_CONSTRAINTS]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "test",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Spark job definition 1')]",
			"type": "Microsoft.Synapse/workspaces/sparkJobDefinitions",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"targetBigDataPool": {
					"referenceName": "testDotNt",
					"type": "BigDataPoolReference"
				},
				"requiredSparkVersion": "3.3",
				"language": "python",
				"scanFolder": false,
				"jobProperties": {
					"name": "Spark job definition 1",
					"file": "abfss://test-synapse1@testsynapse12.dfs.core.windows.net/sparkjob/SampleCode.py",
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "4",
						"spark.dynamicAllocation.maxExecutors": "19",
						"spark.autotune.trackingId": "5380d3d8-1612-47db-b2cf-4e1b3b660ce3",
						"spark.synapse.context.sjdname": "Spark job definition 1"
					},
					"args": [
						"abfss://test-synapse1@testsynapse12.dfs.core.windows.net/data/sample.csv",
						"abfss://test-synapse1@testsynapse12.dfs.core.windows.net/output/"
					],
					"jars": [],
					"pyFiles": [
						""
					],
					"files": [],
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 4
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AccessDB')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "AccessDatabase"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "testmpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7d696ac0-6e3b-4598-a727-ae6b9544cfca"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2/resourceGroups/test-synapse1/providers/Microsoft.Synapse/workspaces/test-synapse1-fan/bigDataPools/testmpool",
						"name": "testmpool",
						"type": "Spark",
						"endpoint": "https://test-synapse1-fan.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/testmpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import col \r\n",
							"\r\n",
							"# Read from existing internal table\r\n",
							"# test-synapse1-fan.sql.azuresynapse.net\r\n",
							"dfToReadFromTable = (spark.read.option(Constants.SERVER, \"mysqlservercup.database.windows.net\")\r\n",
							"    .option(Constants.USER, \"azureuser\")\r\n",
							"    .option(Constants.PASSWORD, \"Pa$$w0rd\")\r\n",
							"    .synapsesql(\"mySampleDatabase.dbo.test1\")\r\n",
							"    .select(\"*\")\r\n",
							"    .limit(2)) \r\n",
							"\r\n",
							"# Show contents of the dataframe\r\n",
							"dfToReadFromTable.show()\r\n",
							"\r\n",
							"print(dfToReadFromTable)\r\n",
							"print(\"==================\")\r\n",
							"dfToReadFromTable.printSchema()\r\n",
							"\r\n",
							"\r\n",
							"print(dfToReadFromTable.limit(10))"
						],
						"outputs": [],
						"execution_count": 3
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CheckODBCDriver')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "testmpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "1bc89c1c-a765-4898-b74c-63c6689503b6"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2/resourceGroups/test-synapse1/providers/Microsoft.Synapse/workspaces/test-synapse1-fan/bigDataPools/testmpool",
						"name": "testmpool",
						"type": "Spark",
						"endpoint": "https://test-synapse1-fan.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/testmpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"pip list"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pyodbc\r\n",
							"\r\n",
							"pyodbc.drivers()"
						],
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Connect to AML')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "testDotNt",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9ab54d80-206d-4f3d-a92b-fccaafacc647"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2/resourceGroups/test-synapse1/providers/Microsoft.Synapse/workspaces/test-synapse1-fan/bigDataPools/testDotNt",
						"name": "testDotNt",
						"type": "Spark",
						"endpoint": "https://test-synapse1-fan.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/testDotNt",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# # pip install azureml-sdk\r\n",
							"# from pyspark.sql import SparkSession\r\n",
							"# from azureml.core import Workspace, Experiment\r\n",
							"# from azureml.core.authentication import ServicePrincipalAuthentication\r\n",
							"# from datetime import datetime\r\n",
							" \r\n",
							"# # Set your Azure subscription ID, resource group, workspace name, and service principal details\r\n",
							"\r\n",
							"# subscription_id = \"f6b7aa7e-08ed-4347-ac30-c9efab8702b2\"\r\n",
							"# resource_group = \"josh-aiml-prod-rg\"\r\n",
							"# workspace_name = \"joshaimlprodws\"\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"from azureml.core import Workspace, Experiment\r\n",
							"from azureml.core.authentication import ServicePrincipalAuthentication\r\n",
							"from datetime import datetime\r\n",
							" \r\n",
							"# Set your Azure subscription ID, resource group, workspace name, and service principal details\r\n",
							"subscription_id = \"cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2\"\r\n",
							"resource_group = \"testmachinelearning\"\r\n",
							"workspace_name = \"testmachinelearning\"\r\n",
							"tenant_id = \"72f988bf-86f1-41af-91ab-2d7cd011db47\"\r\n",
							"client_id = \"231c1695-dd3b-4771-837c-8b887d9933fd\"\r\n",
							"client_secret = \"Ejx8Q~~16zLuHRn2nPryp-NWr3FwHX0L96VMfdeP\"\r\n",
							"\r\n",
							"# Initialize a Spark session\r\n",
							"spark = SparkSession.builder.appName(\"AMLExperimentList\").getOrCreate()\r\n",
							"\r\n",
							"# Authenticate using ServicePrincipalAuthentication\r\n",
							"auth = ServicePrincipalAuthentication(\r\n",
							"    tenant_id=tenant_id,\r\n",
							"    service_principal_id=client_id,\r\n",
							"    service_principal_password=client_secret\r\n",
							")\r\n",
							"\r\n",
							"# Load the Azure Machine Learning workspace\r\n",
							"ws = Workspace(subscription_id=subscription_id, resource_group=resource_group, workspace_name=workspace_name, auth=auth)\r\n",
							"\r\n",
							"# Retrieve the list of experiments\r\n",
							"experiments = Experiment.list(ws)\r\n",
							"\r\n",
							"print(ws)\r\n",
							"print(experiments)\r\n",
							"\r\n",
							"\"\"\"\r\n",
							"# Create a list of experiment names\r\n",
							"#experiment_names = [experiment.name for experiment in experiments]\r\n",
							"\r\n",
							"# Create a DataFrame from the list of experiment names\r\n",
							"#experiment_df = spark.createDataFrame([(name,) for name in experiment_names], [\"ExperimentName\"])\r\n",
							"\r\n",
							"# Show the list of experiments\r\n",
							"#experiment_df.show()\r\n",
							" \r\n",
							"# Specify the name of the experiment you want to list the latest job from\r\n",
							"target_experiment_name = \"prod_ranker_training_pipeline\"\r\n",
							"\r\n",
							"# Get the experiment by name\r\n",
							"target_experiment = None\r\n",
							"for experiment in experiments:\r\n",
							"    if experiment.name == target_experiment_name:\r\n",
							"        target_experiment = experiment\r\n",
							"        break\r\n",
							"\r\n",
							"if target_experiment:\r\n",
							"    # Retrieve runs for the experiment\r\n",
							"    runs = target_experiment.get_runs()\r\n",
							"\r\n",
							"    # Find the latest run based on start_time (as retrieved from run details)\r\n",
							"    latest_run = None\r\n",
							"    latest_start_time = datetime.min\r\n",
							"    for run in runs:\r\n",
							"        run_details = run.get_details()\r\n",
							"        run_start_time_str = run_details.get(\"startTimeUtc\", None)\r\n",
							"        if run_start_time_str:\r\n",
							"            run_start_time = datetime.strptime(run_start_time_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\r\n",
							"            if run_start_time > latest_start_time:\r\n",
							"                latest_run = run\r\n",
							"                latest_start_time = run_start_time\r\n",
							"\r\n",
							"    if latest_run:\r\n",
							"        # Print information about the latest run\r\n",
							"        print(\"Latest Run in Experiment:\", target_experiment.name)\r\n",
							"        print(\"Run ID:\", latest_run.id)\r\n",
							"        print(\"Start Time:\", latest_start_time)\r\n",
							"        print(\"Status:\", latest_run.get_status())\r\n",
							"\r\n",
							"    # Cancel the latest run\r\n",
							"    latest_run.cancel()\r\n",
							"    print(\"Latest run has been canceled.\")\r\n",
							"else:\r\n",
							"    print(\"No runs found in the specified experiment\")\"\r\n",
							"tenant_id = \"e6aea76e-b321-4f9a-a6d0-48132c733f84\"\r\n",
							"client_id = \"73a85c80-0e20-4894-81c9-0e6f3f41a3c5\"\r\n",
							"client_secret = \"wSD8Q~RpI906SK0tcGuE2ClCiWt67Cw0qofEObC5\"\r\n",
							"\r\n",
							"# Initialize a Spark session\r\n",
							"spark = SparkSession.builder.appName(\"AMLExperimentList\").getOrCreate()\r\n",
							"\r\n",
							"# Authenticate using ServicePrincipalAuthentication\r\n",
							"auth = ServicePrincipalAuthentication(\r\n",
							"    tenant_id=tenant_id,\r\n",
							"    service_principal_id=client_id,\r\n",
							"    service_principal_password=client_secret\r\n",
							")\r\n",
							"\r\n",
							"# Load the Azure Machine Learning workspace\r\n",
							"ws = Workspace(subscription_id=subscription_id, resource_group=resource_group, workspace_name=workspace_name, auth=auth)\r\n",
							"\r\n",
							"# Retrieve the list of experiments\r\n",
							"experiments = Experiment.list(ws)\r\n",
							"\r\n",
							"# Create a list of experiment names\r\n",
							"#experiment_names = [experiment.name for experiment in experiments]\r\n",
							"\r\n",
							"# Create a DataFrame from the list of experiment names\r\n",
							"#experiment_df = spark.createDataFrame([(name,) for name in experiment_names], [\"ExperimentName\"])\r\n",
							"\r\n",
							"# Show the list of experiments\r\n",
							"#experiment_df.show()\r\n",
							" \r\n",
							"# Specify the name of the experiment you want to list the latest job from\r\n",
							"target_experiment_name = \"prod_ranker_training_pipeline\"\r\n",
							"\r\n",
							"# Get the experiment by name\r\n",
							"target_experiment = None\r\n",
							"for experiment in experiments:\r\n",
							"    if experiment.name == target_experiment_name:\r\n",
							"        target_experiment = experiment\r\n",
							"        break\r\n",
							"\r\n",
							"if target_experiment:\r\n",
							"    # Retrieve runs for the experiment\r\n",
							"    runs = target_experiment.get_runs()\r\n",
							"\r\n",
							"    # Find the latest run based on start_time (as retrieved from run details)\r\n",
							"    latest_run = None\r\n",
							"    latest_start_time = datetime.min\r\n",
							"    for run in runs:\r\n",
							"        run_details = run.get_details()\r\n",
							"        run_start_time_str = run_details.get(\"startTimeUtc\", None)\r\n",
							"        if run_start_time_str:\r\n",
							"            run_start_time = datetime.strptime(run_start_time_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\r\n",
							"            if run_start_time > latest_start_time:\r\n",
							"                latest_run = run\r\n",
							"                latest_start_time = run_start_time\r\n",
							"\r\n",
							"    if latest_run:\r\n",
							"        # Print information about the latest run\r\n",
							"        print(\"Latest Run in Experiment:\", target_experiment.name)\r\n",
							"        print(\"Run ID:\", latest_run.id)\r\n",
							"        print(\"Start Time:\", latest_start_time)\r\n",
							"        print(\"Status:\", latest_run.get_status())\r\n",
							"\r\n",
							"    # Cancel the latest run\r\n",
							"    latest_run.cancel()\r\n",
							"    print(\"Latest run has been canceled.\")\r\n",
							"else:\r\n",
							"    print(\"No runs found in the specified experiment\")\"\"\""
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from notebookutils.mssparkutils import azureML\r\n",
							"\r\n",
							"# getWorkspace() takes the linked service name,\r\n",
							"# not the Azure Machine Learning workspace name.\r\n",
							"ws = azureML.getWorkspace(\"AzureMLService2\")\r\n",
							"\r\n",
							"print(ws.name)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ConnectwithSQL2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "AccessDatabase"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "testsparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "02c7e5a1-5801-4a9d-b15a-cabccad78837"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1",
						"state": {
							"0228699a-2199-43f9-afa3-47b4f8774b45": {
								"type": "Synapse.DataFrame",
								"sync_state": {
									"table": {
										"rows": [
											{
												"0": "1"
											}
										],
										"schema": [
											{
												"key": "0",
												"name": "col1",
												"type": "int"
											}
										],
										"truncated": false
									},
									"isSummary": false,
									"language": "scala"
								},
								"persist_state": {
									"view": {
										"type": "details",
										"chartOptions": {
											"chartType": "bar",
											"aggregationType": "sum",
											"categoryFieldKeys": [],
											"seriesFieldKeys": [
												"0"
											],
											"isStacked": false
										}
									}
								}
							}
						}
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2/resourceGroups/test-synapse1/providers/Microsoft.Synapse/workspaces/test-synapse1-fan/bigDataPools/testsparkpool",
						"name": "testsparkpool",
						"type": "Spark",
						"endpoint": "https://test-synapse1-fan.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/testsparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"jdbcHostname = \"mysqlservercup.database.windows.net\"  \r\n",
							"jdbcDatabase = \"mySampleDatabase\"  \r\n",
							"jdbcPort = \"1433\"  \r\n",
							"username = \"azureuser@mysqlservercup\"  \r\n",
							"password = \"Pa$$w0rd\"  \r\n",
							"jdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2}\".format(jdbcHostname, jdbcPort, jdbcDatabase)  \r\n",
							"connectionProperties = {  \r\n",
							"  \"user\" : username,  \r\n",
							"  \"password\" : password,  \r\n",
							"  \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"  \r\n",
							"}  \r\n",
							"pushdown_query = \"(select * from sampletbl where col1 > 0) CustomerID\"  \r\n",
							"df = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)  \r\n",
							"display(df) "
						],
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ConnectwithStorage')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "AccessADLSGen2"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "testDotNt",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "33ec1658-0f85-43ac-b3de-70f935eaace9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1",
						"state": {
							"fff8444e-c1fa-4821-8274-846916b1104b": {
								"type": "Synapse.DataFrame",
								"sync_state": {
									"table": {
										"rows": [
											{
												"0": "1",
												"1": "apple"
											},
											{
												"0": "2",
												"1": "banana"
											},
											{
												"0": "3",
												"1": "orange"
											}
										],
										"schema": [
											{
												"key": "0",
												"name": "_c0",
												"type": "string"
											},
											{
												"key": "1",
												"name": "_c1",
												"type": "string"
											}
										],
										"truncated": false
									},
									"isSummary": false,
									"language": "scala"
								},
								"persist_state": {
									"view": {
										"type": "details",
										"chartOptions": {
											"chartType": "bar",
											"aggregationType": "count",
											"categoryFieldKeys": [
												"0"
											],
											"seriesFieldKeys": [
												"0"
											],
											"isStacked": false
										}
									}
								}
							}
						}
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2/resourceGroups/test-synapse1/providers/Microsoft.Synapse/workspaces/test-synapse1-fan/bigDataPools/testDotNt",
						"name": "testDotNt",
						"type": "Spark",
						"endpoint": "https://test-synapse1-fan.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/testDotNt",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\n",
							"blob_account_name = \"amlstragegen2pocje001\"\n",
							"blob_container_name = \"test\"\n",
							"from pyspark.sql import SparkSession\n",
							"\n",
							"sc = SparkSession.builder.getOrCreate()\n",
							"token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\n",
							"blob_sas_token = token_library.getConnectionString(\"ls_amlstragegen2pocje001_test\")\n",
							"\n",
							"spark.conf.set(\n",
							"    'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
							"    blob_sas_token)\n",
							"df = spark.read.load('wasbs://test@amlstragegen2pocje001.blob.core.windows.net/test2/test.parquet', format='parquet')\n",
							"display(df.limit(10))"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"blob_account_name = \"amlstragegen2pocje001\"\r\n",
							"blob_container_name = \"test\"\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"\r\n",
							"sc = SparkSession.builder.getOrCreate()\r\n",
							"token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary\r\n",
							"blob_sas_token = token_library.getConnectionString(\"ls_amlstragegen2pocje001_test\")\r\n",
							"\r\n",
							"spark.conf.set(\r\n",
							"    'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\r\n",
							"    blob_sas_token)\r\n",
							"df = spark.read.load('wasbs://test@amlstragegen2pocje001.blob.core.windows.net/test2/test.parquet', format='parquet')\r\n",
							"display(df.limit(10))"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from notebookutils import mssparkutils\r\n",
							"mssparkutils.fs.ls('abfss://test@fantestmountstorage.dfs.core.windows.net/abc')\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# https://fantestmountstorage.blob.core.windows.net/test/abc/sample.csv\r\n",
							"df = spark.read.load('abfss://test@fantestmountstorage.dfs.core.windows.net/abc/sample.csv',format='csv',header=False)\r\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import Row\r\n",
							"\r\n",
							"df = spark.createDataFrame([\r\n",
							"    Row(a=1, b=4., c='GFG1'),\r\n",
							"   \r\n",
							"    Row(a=2, b=8., c='GFG2'),\r\n",
							"   \r\n",
							"    Row(a=4, b=5., c='GFG3')\r\n",
							"])\r\n",
							" \r\n",
							"# show table\r\n",
							"df.show()\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"test = df.write.mode(\"overwrite\").csv(\"/test/csvfile.csv\")\r\n",
							"print(test)"
						],
						"outputs": [],
						"execution_count": 9
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Ex_Yoyaku_Sakusei_nb')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "6bbbaf76-bcad-4922-8a5d-c651a30bb8d9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import datetime\r\n",
							"import pyspark.sql.functions as F\r\n",
							"from pyspark.sql.types import *"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run modules/common"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 30進法番号取得処理\r\n",
							"@udf(returnType=StringType())\r\n",
							"def base36_encode(number, gymou_dt):\r\n",
							"    try:\r\n",
							"        log('I0001', 'base36_encode')\r\n",
							"\r\n",
							"        # 十進法から30進法へ処理する\r\n",
							"        num_str = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'\r\n",
							"        if number == 0:\r\n",
							"            return '0'\r\n",
							"        base36 = []\r\n",
							"        while number != 0:\r\n",
							"            number, i = divmod(number, 36)\r\n",
							"            base36.append(num_str[i])\r\n",
							"        ret = ''.join(reversed(base36)).zfill(8)\r\n",
							"        ret = gymou_dt + ret\r\n",
							"        \r\n",
							"        log('I0002', 'base36_encode')\r\n",
							"        return ret\r\n",
							"    except Exception as e:\r\n",
							"        log(E0001, 'base36_encode', type(e), e)\r\n",
							"        raise e"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 業務日付情報取得処理\r\n",
							"def get_etl_gyomu_dt():\r\n",
							"    try:\r\n",
							"        log('I0001', 'get_etl_gyomu_dt')\r\n",
							"\r\n",
							"        gyomu_dt = select_gyomu_dt()\r\n",
							"        \r\n",
							"        log('I0002', 'get_etl_gyomu_dt')\r\n",
							"        return gyomu_dt\r\n",
							"    except Exception as e:\r\n",
							"        log(E0001, 'get_etl_gyomu_dt', type(e), e)\r\n",
							"        raise e"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 会員マスタ読み込み処理\r\n",
							"def load_kain_mst():\r\n",
							"    try:\r\n",
							"        log('I0001', 'load_kain_mst')\r\n",
							"\r\n",
							"        # load_table(link_service:str,table_name:str):\r\n",
							"        kaiin_mst_jyoho =load_table('personal_information_sql', 'KAIIN_MST')\r\n",
							"        # カラム選択\r\n",
							"        kaiin_mst_jyoho = kaiin_mst_jyoho.select(col(\"T_CLIENT_NUMBER\"),col(\"KAIIN_ID\"))\r\n",
							"\r\n",
							"        log('I0002', 'load_kain_mst')\r\n",
							"        return kaiin_mst_jyoho\r\n",
							"    except Exception as e:\r\n",
							"        log(E0001, 'load_kain_mst', type(e), e)\r\n",
							"        raise e"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# EX予約実績読み込み処理\r\n",
							"def load_exp_yoyaku_jisseki(gyomu_dt):\r\n",
							"    try:\r\n",
							"        log('I0001', 'load_exp_yoyaku_jisseki')\r\n",
							"\r\n",
							"        # load_table(link_service:str,table_name:str):\r\n",
							"        exp_yoyaku_jisseki_jyoho = load_table('analysis_data_sql', 'EXP_YOYAKU_JISSEKI')\r\n",
							"        # 抽出条件:統合CRM日次処理日(T_CRM_SHORI_YMD)=取得した業務日付(GYOMU_DT)\r\n",
							"        exp_yoyaku_jisseki_jyoho = exp_yoyaku_jisseki_jyoho.where(col('T_CRM_SHORI_YMD') == gyomu_dt)\r\n",
							"        # カラム選択\r\n",
							"        exp_yoyaku_jisseki_jyoho = exp_yoyaku_jisseki_jyoho.select(F.when(col('SOSA_NAIYO')!='予約',None).otherwise(col('SOSA_YMD')).alias('YOYAKU_YMD_JST'),\r\n",
							"                                                                col('JOSHA_YMD').alias('JOSHA_YMD_JST'),\r\n",
							"                                                                col('SOSA_NAIYO'),col('JOSHA_EKI_NAME').alias('HATSU_EKI_NAME'),\r\n",
							"                                                                col('JOSHA_EKI_CD').alias('HATSU_EKI_CD_3'),col('KOSHA_EKI_NAME').alias('CHAKU_EKI_NAME'),\r\n",
							"                                                                col('KOSHA_EKI_CD').alias('CHAKU_EKI_CD_3'),col('KAIIN_ID'),col('T_CRM_SHORI_YMD'))\r\n",
							"        log('I0002', 'load_exp_yoyaku_jisseki')\r\n",
							"        return exp_yoyaku_jisseki_jyoho\r\n",
							"    except Exception as e:\r\n",
							"        log(E0001, 'load_exp_yoyaku_jisseki', type(e), e)\r\n",
							"        raise e"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 会員マスタとEX予約実績情報の結合処理\r\n",
							"def exp_yoyaku_jisseki_left_join_kaiin_mst(kaiin_mst_jyoho,exp_yoyaku_jisseki_jyoho):\r\n",
							"    try:\r\n",
							"        log('I0001', 'exp_yoyaku_jisseki_left_join_kaiin_mst')\r\n",
							"\r\n",
							"        # 引数「kaiin_mst_jyoho」に引数「exp_yoyaku_jisseki_jyoho」を左外部結合し、変数「kaiin_and_yoyaku_jyoho」に設定する。\r\n",
							"        join_condition =[ kaiin_mst_jyoho.KAIIN_ID == exp_yoyaku_jisseki_jyoho.KAIIN_ID]\r\n",
							"        kaiin_and_yoyaku_jyoho = kaiin_mst_jyoho.join(exp_yoyaku_jisseki_jyoho, join_condition, 'left')\r\n",
							"        # TODO \r\n",
							"        kaiin_and_yoyaku_jyoho = kaiin_and_yoyaku_jyoho.drop(exp_yoyaku_jisseki_jyoho.KAIIN_ID)\r\n",
							"\r\n",
							"        log('I0002', 'exp_yoyaku_jisseki_left_join_kaiin_mst')\r\n",
							"        return kaiin_and_yoyaku_jyoho\r\n",
							"    except Exception as e:\r\n",
							"        log(E0001, 'exp_yoyaku_jisseki_left_join_kaiin_mst', type(e), e)\r\n",
							"        raise e"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 採番処理\r\n",
							"def set_saiban(kaiin_and_yoyaku_jyoho,gymou_dt):\r\n",
							"    try:\r\n",
							"        log('I0001', 'set_saiban')\r\n",
							"\r\n",
							"        # 採番方法を実行し、カラム「連番」を「kaiin_and_yoyaku_jyoho」に挿入し、結果を「kaiin_and_yoyaku_jyoho」に設定する\r\n",
							"        w = Window().orderBy(lit('A'))\r\n",
							"        kaiin_and_yoyaku_jyoho = kaiin_and_yoyaku_jyoho.withColumn(\"row_num\", row_number().over(w))\r\n",
							"        kaiin_and_yoyaku_jyoho = kaiin_and_yoyaku_jyoho.withColumn(\"RENBAN\", base36_encode(col('row_num'), lit(gyomu_dt)))\r\n",
							"        \r\n",
							"        log('I0002', 'set_saiban')\r\n",
							"        return kaiin_and_yoyaku_jyoho\r\n",
							"    except Exception as e:\r\n",
							"        log(E0001, 'set_saiban', type(e), e)\r\n",
							"        raise e"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 4056_（マスタ）e5489駅マスタ読み込み処理\r\n",
							"def load_tdc_e5489_eki():\r\n",
							"    try:\r\n",
							"        log('I0001', 'load_tdc_e5489_eki')\r\n",
							"\r\n",
							"        # load_table(link_service:str,table_name:str):\r\n",
							"        tdc_e5489_eki_jyoho = load_table('analysis_data_sql', 'TDC_E5489_EKI')\r\n",
							"        \r\n",
							"        log('I0002', 'load_tdc_e5489_eki')\r\n",
							"        return tdc_e5489_eki_jyoho\r\n",
							"    except Exception as e:\r\n",
							"        log(E0001, 'load_tdc_e5489_eki', type(e), e)\r\n",
							"        raise e"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# EX予約実績情報と4056_（マスタ）e5489駅マスタの結合処理\r\n",
							"def exp_yoyaku_jisseki_join_tdc_e5489_eki(kaiin_and_yoyaku_jyoho,tdc_e5489_eki_jyoho):\r\n",
							"    try:\r\n",
							"        log('I0001', 'exp_yoyaku_jisseki_join_tdc_e5489_eki')\r\n",
							"\r\n",
							"        # 左外部結合し、変数「jisseki_join_e5489_jyoho」に設定する。乗車駅コード（12桁）の取得\r\n",
							"        join_condition = [kaiin_and_yoyaku_jyoho.HATSU_EKI_CD_3 == tdc_e5489_eki_jyoho.SHINKANSEN_EKI_CD]\r\n",
							"        jisseki_join_e5489_jyoho = kaiin_and_yoyaku_jyoho.join(tdc_e5489_eki_jyoho, join_condition, 'left')\r\n",
							"        # カラム選択\r\n",
							"        jisseki_join_e5489_jyoho = jisseki_join_e5489_jyoho.select(col('T_CLIENT_NUMBER'),col('RENBAN'),col('YOYAKU_YMD_JST'),col('JOSHA_YMD_JST'),col('SOSA_NAIYO'),col('HATSU_EKI_NAME'),\r\n",
							"                                    col('HATSU_EKI_CD_3'),col('CHAKU_EKI_NAME'),col('CHAKU_EKI_CD_3'),col('EKI_CD').alias('HATSU_EKI_CD_12'))    \r\n",
							"        \r\n",
							"        # 左外部結合し、変数「jisseki_join_e5489_jyoho」に設定する。到着駅コード（12桁）の取得\r\n",
							"        join_condition = [jisseki_join_e5489_jyoho.CHAKU_EKI_CD_3==tdc_e5489_eki_jyoho.SHINKANSEN_EKI_CD]\r\n",
							"        jisseki_join_e5489_jyoho = jisseki_join_e5489_jyoho.join(tdc_e5489_eki_jyoho, join_condition, 'left')\r\n",
							"\r\n",
							"        # カラム選択\r\n",
							"        jisseki_join_e5489_jyoho = jisseki_join_e5489_jyoho.select(col('T_CLIENT_NUMBER'),col('RENBAN'),col('YOYAKU_YMD_JST'),col('JOSHA_YMD_JST'),col('SOSA_NAIYO'),col('HATSU_EKI_NAME'),\r\n",
							"                                    col('HATSU_EKI_CD_3'),col('CHAKU_EKI_NAME'),col('CHAKU_EKI_CD_3'),col('HATSU_EKI_CD_12'),col('EKI_CD').alias('CHAKU_EKI_CD_12'))    \r\n",
							"\r\n",
							"        log('I0002', 'exp_yoyaku_jisseki_join_tdc_e5489_eki')\r\n",
							"        return jisseki_join_e5489_jyoho\r\n",
							"    except Exception as e:\r\n",
							"        log(E0001, 'exp_yoyaku_jisseki_join_tdc_e5489_eki', type(e), e)\r\n",
							"        raise e"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# データ加工処理\r\n",
							"def edit_data(jisseki_join_e5489_jyoho,gyomu_dt):\r\n",
							"    try:\r\n",
							"        log('I0001', 'edit_data')\r\n",
							"\r\n",
							"        # カラムを挿入する\r\n",
							"        edit_data = jisseki_join_e5489_jyoho.withColumn('T_CRM_SHORI_YMD',lit(gyomu_dt))\r\n",
							"        edit_data = edit_data.withColumn('T_CRM_UPDATE_DT', F.current_timestamp())\r\n",
							"        edit_data = edit_data.withColumn('T_CRM_UPDATE_ID', lit('CRM_ETL_18_01_01'))\r\n",
							"        edit_data = edit_data.withColumn('MA_DATA_KOUSHIN_DT_JST', F.current_timestamp())\r\n",
							"\r\n",
							"        log('I0002', 'edit_data')\r\n",
							"        return edit_data\r\n",
							"    except Exception as e:\r\n",
							"        log(E0001, 'edit_data', type(e), e)\r\n",
							"        raise e"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# EX予約読み込み処理\r\n",
							"def load_ex_yoyaku():\r\n",
							"    try:\r\n",
							"        log('I0001', 'load_ex_yoyaku')\r\n",
							"\r\n",
							"        # load_table(link_service:str,table_name:str):\r\n",
							"        exp_yoyaku_jyoho  =load_table('analysis_data_sql', 'EX_YOYAKU')\r\n",
							"\r\n",
							"        log('I0002', 'load_ex_yoyaku')\r\n",
							"        return exp_yoyaku_jyoho\r\n",
							"    except Exception as e:\r\n",
							"        log(E0001, 'load_ex_yoyaku', type(e), e)\r\n",
							"        raise e"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 登録データ加工処理\r\n",
							"def edit_insert_data(edit_data,exp_yoyaku_jyoho):\r\n",
							"    try:\r\n",
							"        log('I0001', 'edit_insert_data')\r\n",
							"\r\n",
							"        # edit_dataから登録データを取得する\r\n",
							"        join_condition = [(edit_data.T_CLIENT_NUMBER == exp_yoyaku_jyoho.T_CLIENT_NUMBER) & (edit_data.RENBAN == exp_yoyaku_jyoho.RENBAN)]\r\n",
							"        insert_data = edit_data.join(exp_yoyaku_jyoho, join_condition, 'left')\r\n",
							"        insert_data = insert_data.where(exp_yoyaku_jyoho.T_CLIENT_NUMBER.isNull())\r\n",
							"        insert_data = insert_data.select(edit_data.T_CLIENT_NUMBER,\r\n",
							"                                        edit_data.RENBAN,\r\n",
							"                                        edit_data.YOYAKU_YMD_JST,\r\n",
							"                                        edit_data.SOSA_NAIYO,\r\n",
							"                                        edit_data.JOSHA_YMD_JST,\r\n",
							"                                        edit_data.HATSU_EKI_NAME,\r\n",
							"                                        edit_data.HATSU_EKI_CD_3,\r\n",
							"                                        edit_data.HATSU_EKI_CD_12,\r\n",
							"                                        edit_data.CHAKU_EKI_NAME,\r\n",
							"                                        edit_data.CHAKU_EKI_CD_3,\r\n",
							"                                        edit_data.CHAKU_EKI_CD_12,\r\n",
							"                                        edit_data.T_CRM_SHORI_YMD,\r\n",
							"                                        edit_data.T_CRM_UPDATE_DT,\r\n",
							"                                        edit_data.T_CRM_UPDATE_ID,\r\n",
							"                                        edit_data.MA_DATA_KOUSHIN_DT_JST)\r\n",
							"\r\n",
							"        log('I0002', 'edit_insert_data')\r\n",
							"        return insert_data\r\n",
							"    except Exception as e:\r\n",
							"        log(E0001, 'edit_insert_data', type(e), e)\r\n",
							"        raise e"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 更新データ加工処理\r\n",
							"def edit_update_data(edit_data,exp_yoyaku_jyoho):\r\n",
							"    try:\r\n",
							"        log('I0001', 'edit_update_data')\r\n",
							"\r\n",
							"        # edit_dataから更新データを取得する\r\n",
							"        join_condition = [(edit_data.T_CLIENT_NUMBER == exp_yoyaku_jyoho.T_CLIENT_NUMBER) & (edit_data.RENBAN == exp_yoyaku_jyoho.RENBAN)]\r\n",
							"        update_data = edit_data.join(exp_yoyaku_jyoho, join_condition, 'inner')\r\n",
							"        update_data = update_data.select(edit_data.T_CLIENT_NUMBER,\r\n",
							"                                        edit_data.RENBAN,\r\n",
							"                                        edit_data.YOYAKU_YMD_JST,\r\n",
							"                                        edit_data.SOSA_NAIYO,\r\n",
							"                                        edit_data.JOSHA_YMD_JST,\r\n",
							"                                        edit_data.HATSU_EKI_NAME,\r\n",
							"                                        edit_data.HATSU_EKI_CD_3,\r\n",
							"                                        edit_data.HATSU_EKI_CD_12,\r\n",
							"                                        edit_data.CHAKU_EKI_NAME,\r\n",
							"                                        edit_data.CHAKU_EKI_CD_3,\r\n",
							"                                        edit_data.CHAKU_EKI_CD_12,\r\n",
							"                                        edit_data.T_CRM_SHORI_YMD,\r\n",
							"                                        edit_data.T_CRM_UPDATE_DT,\r\n",
							"                                        edit_data.T_CRM_UPDATE_ID,\r\n",
							"                                        edit_data.MA_DATA_KOUSHIN_DT_JST)\r\n",
							"        # 更新SQL作成\r\n",
							"        update_data = update_data.withColumn(\"update_sql\", F.format_string(\"update EX_YOYAKU set YOYAKU_YMD_JST=%s,\"\r\n",
							"                                                                                                 \"SOSA_NAIYO=%s,\"\r\n",
							"                                                                                                 \"JOSHA_YMD_JST=%s,\"\r\n",
							"                                                                                                 \"HATSU_EKI_NAME=%s,\"\r\n",
							"                                                                                                 \"HATSU_EKI_CD_3=%s,\"\r\n",
							"                                                                                                 \"HATSU_EKI_CD_12=%s,\"\r\n",
							"                                                                                                 \"CHAKU_EKI_NAME=%s,\"\r\n",
							"                                                                                                 \"CHAKU_EKI_CD_3=%s,\"\r\n",
							"                                                                                                 \"CHAKU_EKI_CD_12=%s,\"\r\n",
							"                                                                                                 \"T_CRM_SHORI_YMD=%s,\"\r\n",
							"                                                                                                 \"T_CRM_UPDATE_DT=%s,\"\r\n",
							"                                                                                                 \"T_CRM_UPDATE_ID=%s,\"\r\n",
							"                                                                                                 \"MA_DATA_KOUSHIN_DT_JST=%s\"\r\n",
							"                                                                                                 \" where T_CLIENT_NUMBER = %s and RENBAN=%s\",col('YOYAKU_YMD_JST'),\r\n",
							"                                                                                                 col('SOSA_NAIYO'),\r\n",
							"                                                                                                 col('JOSHA_YMD_JST'),\r\n",
							"                                                                                                 col('HATSU_EKI_NAME'),\r\n",
							"                                                                                                 col('HATSU_EKI_CD_3'),\r\n",
							"                                                                                                 col('HATSU_EKI_CD_12'),\r\n",
							"                                                                                                 col('CHAKU_EKI_NAME'),\r\n",
							"                                                                                                 col('CHAKU_EKI_CD_3'),\r\n",
							"                                                                                                 col('CHAKU_EKI_CD_12'),\r\n",
							"                                                                                                 col('T_CRM_SHORI_YMD'),\r\n",
							"                                                                                                 col('T_CRM_UPDATE_DT'),\r\n",
							"                                                                                                 col('T_CRM_UPDATE_ID'),\r\n",
							"                                                                                                 col('MA_DATA_KOUSHIN_DT_JST'),col('T_CLIENT_NUMBER'),col('RENBAN')\r\n",
							"                                                                                                 ))    \r\n",
							"    \r\n",
							"        log('I0002', 'edit_update_data')\r\n",
							"        return update_data\r\n",
							"    except Exception as e:\r\n",
							"        log(E0001, 'edit_update_data', type(e), e)\r\n",
							"        raise e"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 登録データ保存処理\r\n",
							"def save_insert_data(insert_data):\r\n",
							"    try:\r\n",
							"        log('I0001', 'save_insert_data')\r\n",
							"\r\n",
							"        # save_table(link_service:str,table_name:str,data: DataFrame):\r\n",
							"        save_table('analysis_data_sql', 'EX_YOYAKU',insert_data)\r\n",
							"\r\n",
							"        log('I0002', 'save_insert_data')\r\n",
							"        \r\n",
							"    except Exception as e:\r\n",
							"        log(E0001, 'save_insert_data', type(e), e)\r\n",
							"        raise e"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 更新データ保存処理\r\n",
							"def save_update_data(update_data):\r\n",
							"    try:\r\n",
							"        log('I0001', 'save_update_data')\r\n",
							"\r\n",
							"        # executeQuery関数（クエリ実行共通関数）\r\n",
							"        for row in update_data.collect():\r\n",
							"            update_sql = row.update_sql\r\n",
							"            executeQuery('analysis_data_sql', update_sql)\r\n",
							"        \r\n",
							"        log('I0002', 'save_update_data')\r\n",
							"        \r\n",
							"    except Exception as e:\r\n",
							"        log(E0001, 'save_update_data', type(e), e)\r\n",
							"        raise e"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# EX予約情報抽出処理\r\n",
							"def select_ex_yoyaku(gyomu_dt):\r\n",
							"    try:\r\n",
							"        log('I0001', 'select_ex_yoyaku')\r\n",
							"\r\n",
							"        # load_table(link_service:str,table_name:str):\r\n",
							"        output_yoyaku_jyoho  =load_table('analysis_data_sql', 'EX_YOYAKU')\r\n",
							"\r\n",
							"        # 条件に合わせて、データを抽出\r\n",
							"        output_yoyaku_jyoho = output_yoyaku_jyoho.where((col('T_CRM_SHORI_YMD') == gyomu_dt) & (col('SOSA_NAIYO') == '購入'))\r\n",
							"\r\n",
							"        # カラム選択\r\n",
							"        output_data = output_yoyaku_jyoho.select(col('T_CLIENT_NUMBER'),col('RENBAN'),col('YOYAKU_YMD_JST'),col('JOSHA_YMD_JST'),col('HATSU_EKI_NAME'),\r\n",
							"        col('HATSU_EKI_CD_3'),col('HATSU_EKI_CD_12'),col('CHAKU_EKI_NAME'),col('CHAKU_EKI_CD_3'),col('CHAKU_EKI_CD_12'),col('MA_DATA_KOUSHIN_DT_JST'))\r\n",
							"        \r\n",
							"        log('I0002', 'select_ex_yoyaku')\r\n",
							"        return output_data\r\n",
							"    except Exception as e:\r\n",
							"        log(E0001, 'select_ex_yoyaku', type(e), e)\r\n",
							"        raise e"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# 出力データ保存処理\r\n",
							"def save_output_data(output_data,gymou_dt):\r\n",
							"    try:\r\n",
							"        log('I0001', 'save_output_data')\r\n",
							"\r\n",
							"        file_type = 'tsv'\r\n",
							"        mode = 'w'\r\n",
							"        file_path =f'Export/System/EX_YOYAKU_{gymou_dt}'\r\n",
							"\r\n",
							"        # save_sftp(link_service:str,file_path:str,data:DataFrame,file_type:str,mode:str):\r\n",
							"        save_sftp('analysis_data_sql',file_path,output_data,file_type,mode)\r\n",
							"        \r\n",
							"        log('I0002', 'save_output_data')\r\n",
							"        \r\n",
							"    except Exception as e:\r\n",
							"        log(E0001, 'save_output_data', type(e), e)\r\n",
							"        raise e"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# メイン処理\r\n",
							"if __name__ == '__main__':\r\n",
							"    try:\r\n",
							"        log('I0001', 'main')\r\n",
							"\r\n",
							"        # 業務日付情報取得処理実行\r\n",
							"        gyomu_dt = get_etl_gyomu_dt()\r\n",
							"\r\n",
							"        # 会員マスタ読み込み処理実行\r\n",
							"        kaiin_mst_jyoho = load_kain_mst()\r\n",
							"\r\n",
							"        # EX予約実績読み込み処理実行\r\n",
							"        exp_yoyaku_jisseki_jyoho = load_exp_yoyaku_jisseki(gymou_dt)\r\n",
							"\r\n",
							"        # 会員マスタとEX予約実績情報の結合処理実行\r\n",
							"        kaiin_and_yoyaku_jyoho = exp_yoyaku_jisseki_left_join_kaiin_mst(kaiin_mst_jyoho,exp_yoyaku_jisseki_jyoho)\r\n",
							"\r\n",
							"        # 採番処理実行\r\n",
							"        kaiin_and_yoyaku_jyoho =set_saiban(kaiin_and_yoyaku_jyoho,gymou_dt)\r\n",
							"\r\n",
							"        # 4056_（マスタ）e5489駅マスタ読み込み処理実行\r\n",
							"        tdc_e5489_eki_jyoho =load_tdc_e5489_eki()\r\n",
							"\r\n",
							"        # EX予約実績情報と4056_（マスタ）e5489駅マスタの結合処理\r\n",
							"        jisseki_join_e5489_jyoho = exp_yoyaku_jisseki_join_tdc_e5489_eki(kaiin_and_yoyaku_jyoho,tdc_e5489_eki_jyoho)\r\n",
							"\r\n",
							"        # データ加工処理\r\n",
							"        edit_data = exp_yoyaku_jisseki_join_tdc_e5489_eki(jisseki_join_e5489_jyoho,gyomu_dt)\r\n",
							"\r\n",
							"        # EX予約読み込み処理実行\r\n",
							"        exp_yoyaku_jyoho = load_ex_yoyaku()\r\n",
							"\r\n",
							"        # 登録データ加工処理実行\r\n",
							"        insert_data =edit_insert_data(edit_data,exp_yoyaku_jyoho)\r\n",
							"\r\n",
							"        # 更新データ加工処理実行\r\n",
							"        update_data =edit_update_data(edit_data,exp_yoyaku_jyoho)\r\n",
							"\r\n",
							"        # 登録データ保存処理実行\r\n",
							"        save_insert_data(insert_data)\r\n",
							"\r\n",
							"        # 更新データ保存処理実行\r\n",
							"        save_update_data(update_data)\r\n",
							"\r\n",
							"        # EX予約情報抽出処理\r\n",
							"        output_data = select_ex_yoyaku()\r\n",
							"\r\n",
							"        # 出力データ保存処理\r\n",
							"        save_output_data(output_data,gymou_dt)\r\n",
							"\r\n",
							"        log('I0002', 'main')   \r\n",
							"    except Exception as e:\r\n",
							"        log(E0001, 'main', type(e), e)\r\n",
							"        raise e\r\n",
							""
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 11')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "AccessADLSGen2"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "testsparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "f0152e79-8e17-4b7a-aa7b-458c5f4dec06"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2/resourceGroups/test-synapse1/providers/Microsoft.Synapse/workspaces/test-synapse1-fan/bigDataPools/testsparkpool",
						"name": "testsparkpool",
						"type": "Spark",
						"endpoint": "https://test-synapse1-fan.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/testsparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"data_path = spark.read.load('abfss://excel@testfansynapseadlsgen2.dfs.core.windows.net/sample.csv', format='csv', header=False)\r\n",
							"data_path.show(10)\r\n",
							"\r\n",
							"print('Converting to Pandas.')\r\n",
							"\r\n",
							"pdf = data_path.toPandas()\r\n",
							"print(pdf)"
						],
						"outputs": [],
						"execution_count": 49
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pandas\r\n",
							"\r\n",
							"\r\n",
							"df = pandas.read_csv('abfss://excel@testfansynapseadlsgen2.dfs.core.windows.net/sample.csv')\r\n",
							"print(df)\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"# # abfss://test4synapse@testsynapseprivatefan.dfs.core.windows.net/sample.csv\r\n",
							"# import pandas\r\n",
							"\r\n",
							"# df = pandas.read_csv('abfss://excel@testfansynapseadlsgen2.dfs.core.windows.net/Book1.xlsx',\r\n",
							"# storage_options={'account_key':'VGzDHW00+SB7Bn28DvbS04YQRvZ79kX1tqqTvSTOL9uX/Q+zBFWuDH6cNowHxDweDSBU7NImBuSx+AStJ8suJA=='})"
						],
						"outputs": [],
						"execution_count": 50
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df = pandas.read_excel('abfss://excel@testfansynapseadlsgen2.dfs.core.windows.net/Book1.xlsx')\r\n",
							"print(df)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from openpyxl import load_workbook\r\n",
							"# , read_only=False, keep_vba=True\r\n",
							"\r\n",
							"wb = load_workbook(filename='abfss://excel@testfansynapseadlsgen2.dfs.core.windows.net/sample.csv')\r\n",
							"print(wb)"
						],
						"outputs": [],
						"execution_count": 52
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from openpyxl import load_workbook\r\n",
							"# , read_only=False, keep_vba=True\r\n",
							"# https://testfansynapseadlsgen2.dfs.core.windows.net/excel/Book1.xlsx\r\n",
							"\r\n",
							"wb = load_workbook(filename='https://testfansynapseadlsgen2.dfs.core.windows.net/excel/Book1.xlsx')\r\n",
							"print(wb)"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# testfansynapseadlsgen2.blob.core.windows.net/excel\r\n",
							"mssparkutils.fs.mount( \r\n",
							"    \"abfss://excel@testfansynapseadlsgen2.dfs.core.windows.net/abc\", \r\n",
							"    \"/test2\"\r\n",
							") "
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.fs.ls(\"/test2\")"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from openpyxl import load_workbook\r\n",
							"# , read_only=False, keep_vba=True\r\n",
							"\r\n",
							"wb = load_workbook(filename='/test/Book1.xlsx')\r\n",
							"print(wb)"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# abfss://excel@testfansynapseadlsgen2.dfs.core.windows.net/Book1.xlsx\r\n",
							"mssparkutils.fs.ls(\"abfss://excel@testfansynapseadlsgen2.dfs.core.windows.net/Book1.xlsx\")\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from openpyxl import load_workbook\r\n",
							"# , read_only=False, keep_vba=True\r\n",
							"\r\n",
							"wb = load_workbook(filename=\"https://testfansynapseadlsgen2.blob.core.windows.net/excel/Book1.xlsx?sp=r&st=2023-05-26T11:48:06Z&se=2023-05-31T19:48:06Z&sv=2022-11-02&sr=b&sig=IaBLqs4bmI70s%2FvCQvdBzkoX1Ra5FeYqPw%2BU%2BSylW9o%3D\", read_only=False, keep_vba=True)\r\n",
							"print(wb)"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df = pandas.read_excel('abfss://excel@testfansynapseadlsgen2.dfs.core.windows.net/Book1.xlsx')\r\n",
							"print(df)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.fs.mount(\r\n",
							"   \"abfss://excel@testfansynapseadlsgen2.dfs.core.windows.net/\",\r\n",
							"  \"/mnt/test\")"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.fs.ls(\"//mnt/test/\")\r\n",
							"\r\n",
							"from openpyxl import load_workbook\r\n",
							"# , read_only=False, keep_vba=True\r\n",
							"\r\n",
							"wb = load_workbook(filename=\"//mnt/test/Book1.xlsx\", read_only=False, keep_vba=True)\r\n",
							"print(wb)"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.fs.mount(\r\n",
							"   \"abfss://excel@testfansynapseadlsgen2.dfs.core.windows.net/\",\r\n",
							"  \"/test\")"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.fs.getMountPath(\"/test\") "
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from openpyxl import load_workbook\r\n",
							"\r\n",
							"wb = load_workbook(mssparkutils.fs.getMountPath(\"/test\")+\"/Book1.xlsx\", read_only=False, keep_vba=True)\r\n",
							"\r\n",
							"print(wb.get_sheet_names())"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pandas\r\n",
							"df = pandas.read_excel('abfss://excel@testfansynapseadlsgen2.dfs.core.windows.net/Book1.xlsx')\r\n",
							"print(df)"
						],
						"outputs": [],
						"execution_count": 8
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 12')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "AccessDatabase"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "testmpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "4ea93d8c-3700-441f-bc56-d5dd2d77360e"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2/resourceGroups/test-synapse1/providers/Microsoft.Synapse/workspaces/test-synapse1-fan/bigDataPools/testmpool",
						"name": "testmpool",
						"type": "Spark",
						"endpoint": "https://test-synapse1-fan.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/testmpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"synapsesql(table_name: str=\"\") -> org.apache.spark.sql.DataFrame"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import col\r\n",
							"\r\n",
							"\r\n",
							"\r\n",
							"# jdbc:sqlserver://test-synapse1-fan.sql.azuresynapse.net:1433;database=SynapseDedicatedSQLPool;user=sqladminuser@test-synapse1-fan;password={your_password_here};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\r\n",
							"\r\n",
							"# Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"# Database can be specified as a Spark Config or as a Constant - Constants.DATABASE\r\n",
							"spark.conf.set(\"spark.sqlanalyticsconnector.dw.database\", \"SynapseDedicatedSQLPool\")\r\n",
							"\r\n",
							"# Read from a query\r\n",
							"# Query can be provided either as an argument to synapsesql or as a Constant - Constants.QUERY\r\n",
							"dfToReadFromQueryAsOption = (spark.read\r\n",
							"                     # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"                     # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
							"                     .option(Constants.DATABASE, \"SynapseDedicatedSQLPool\")\r\n",
							"                     # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							"                     # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							"                     .option(Constants.SERVER, \"test-synapse1-fan.sql.azuresynapse.net\")\r\n",
							"                     # Set database user name\r\n",
							"                     .option(Constants.USER, \"sqladminuser\")\r\n",
							"                     # Set user's password to the database\r\n",
							"                     .option(Constants.PASSWORD, \"Pa$$w0rd1234\")\r\n",
							"                     # Set name of the data source definition that is defined with database scoped credentials.\r\n",
							"                     # https://docs.microsoft.com/sql/t-sql/statements/create-external-data-source-transact-sql?view=sql-server-ver15&tabs=dedicated#h-create-external-data-source-to-access-data-in-azure-storage-using-the-abfs-interface\r\n",
							"                     # Data extracted from the SQL query will be staged to the storage path defined on the data source's location setting.\r\n",
							"                     .option(Constants.DATA_SOURCE, \"MyAzureStorage\")\r\n",
							"                     # Query from where data will be read.\r\n",
							"                     .option(Constants.QUERY, \"select * from dbo.Persons \")\r\n",
							"                     .synapsesql()\r\n",
							"                    )\r\n",
							"\r\n",
							"# dfToReadFromQueryAsArgument = (spark.read\r\n",
							"#                      # Name of the SQL Dedicated Pool or database where to run the query\r\n",
							"#                      # Database can be specified as a Spark Config - spark.sqlanalyticsconnector.dw.database or as a Constant - Constants.DATABASE\r\n",
							"#                      .option(Constants.DATABASE, \"<database_name>\")\r\n",
							"#                      # If `Constants.SERVER` is not provided, the `<database_name>` from the three-part table name argument\r\n",
							"#                      # to `synapsesql` method is used to infer the Synapse Dedicated SQL End Point.\r\n",
							"#                      .option(Constants.SERVER, \"<sql-server-name>.sql.azuresynapse.net\")\r\n",
							"#                      # Set database user name\r\n",
							"#                      .option(Constants.USER, \"<user_name>\")\r\n",
							"#                      # Set user's password to the database\r\n",
							"#                      .option(Constants.PASSWORD, \"<user_password>\")\r\n",
							"#                      # Set name of the data source definition that is defined with database scoped credentials.\r\n",
							"#                      # https://docs.microsoft.com/sql/t-sql/statements/create-external-data-source-transact-sql?view=sql-server-ver15&tabs=dedicated#h-create-external-data-source-to-access-data-in-azure-storage-using-the-abfs-interface\r\n",
							"#                      # Data extracted from the SQL query will be staged to the storage path defined on the data source's location setting.\r\n",
							"#                      .option(Constants.DATA_SOURCE, \"<data_source_name>\")\r\n",
							"#                      .synapsesql(\"select <column_name>, count(*) as counts from <schema_name>.<table_name> group by <column_name>\")\r\n",
							"#                     )\r\n",
							"\r\n",
							"# Show contents of the dataframe\r\n",
							"dfToReadFromQueryAsOption.show()\r\n",
							"# dfToReadFromQueryAsArgument.show()"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 14')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "AccessADLSGen2"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "testmpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b553763a-14ec-438a-8aa0-7d62d6c0ddcc"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2/resourceGroups/test-synapse1/providers/Microsoft.Synapse/workspaces/test-synapse1-fan/bigDataPools/testmpool",
						"name": "testmpool",
						"type": "Spark",
						"endpoint": "https://test-synapse1-fan.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/testmpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# -------------------------------------------------------\r\n",
							"# ローデータとデータマートの突合テスト\r\n",
							"#   ローデータとデータマートについて、下記の突合テストを実施する。\r\n",
							"#    1. 行数\r\n",
							"#    2. 列数\r\n",
							"#    3. データの値\r\n",
							"# -------------------------------------------------------\r\n",
							"\r\n",
							"from notebookutils import mssparkutils\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.utils import AnalysisException\r\n",
							"import argparse\r\n",
							"import configparser\r\n",
							"import logging\r\n",
							"import datetime\r\n",
							"from pyspark.sql.window import Window\r\n",
							"import pyspark.sql.functions as F\r\n",
							"import re\r\n",
							"from decimal import Decimal\r\n",
							"\r\n",
							"# -----------------------------------------------------\r\n",
							"# \r\n",
							"# Sparkセッションの作成\r\n",
							"# \r\n",
							"# -----------------------------------------------------\r\n",
							"spark = (SparkSession\r\n",
							"\t.builder\r\n",
							"\t.appName(\"837App\")\r\n",
							"\t.config(\"spark.network.timeout\", \"600s\")\r\n",
							"\t.config(\"spark.executor.heartbeatInterval\", \"10s\")\r\n",
							"#\t.config(\"spark.kryoserializer.buffer.mb\", '32855481')\r\n",
							"\t.config(\"spark.kryoserializer.buffer.max\", \"128m\")\r\n",
							"\t.getOrCreate());\r\n",
							"\r\n",
							"# -----------------------------------------------------\r\n",
							"# \r\n",
							"# ロガーの作成\r\n",
							"# \r\n",
							"# -----------------------------------------------------\r\n",
							"logger  = logging.getLogger(__name__)\r\n",
							"handler = logging.StreamHandler()\r\n",
							"handler.setLevel(logging.INFO)\r\n",
							"handler.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] %(message)s'))\r\n",
							"logger.setLevel(logging.INFO)\r\n",
							"logger.addHandler(handler)\r\n",
							"logger.propagate = False\r\n",
							"\r\n",
							"def spark_read_csv(path, sep, header=True):\r\n",
							"\t\"\"\"\r\n",
							"\tSparkを使ってADLSGen2上のCSV/TSVファイルを読み込み、\r\n",
							"\tSparkDataFrameにして返す。\r\n",
							"\r\n",
							"\tKeyword arguments:\r\n",
							"\tlogger -- ロガー\r\n",
							"\tpath   -- ダウンロード対象のADLSGen2URL。\r\n",
							"\tsep    -- 区切り文字。tsv=r'\\t',csv=','。\r\n",
							"\theader -- ヘッダー。 (default True)\r\n",
							"\t\"\"\"\r\n",
							"\ttry:\r\n",
							"\t\tlogger.info(\"CSV/TSV形式ファイルデータの取得を開始します。[path:{} sep:{} header:{}]\".format(path, sep, header))\r\n",
							"\t\t# CSV・TSV形式ファイルの取得\r\n",
							"\t\tdf = spark.read\\\r\n",
							"\t\t\t.option(\"multiline\", \"true\")\\\r\n",
							"\t\t\t.csv(\r\n",
							"\t\t\t\tpath, \r\n",
							"\t\t\t\tsep=sep, \r\n",
							"\t\t\t\theader=header)\r\n",
							"\t\tlogger.info(\"CSV/TSV形式ファイルデータデータの取得が完了しました。[データ:{}]\".format(df))\r\n",
							"\t\treturn df\r\n",
							"\texcept AnalysisException as e:\r\n",
							"\t\tlogger.error(\"パスにファイルが存在しません。[パス:{} e:{}]\".format(path, e))\r\n",
							"\t\traise\r\n",
							"\texcept Exception as e:\r\n",
							"\t\tlogger.error(\"データ取得に失敗しました。[パス:{} e:{}]\".format(path, e))\r\n",
							"\t\traise\r\n",
							"\r\n",
							"def spark_read_parquet(path, header=True):\r\n",
							"\t\"\"\"\r\n",
							"\tSparkを使ってADLSGen2上のparquetファイルを読み込み、\r\n",
							"\tSparkDataFrameにして返す。\r\n",
							"\r\n",
							"\tKeyword arguments:\r\n",
							"\tlogger -- ロガー\r\n",
							"\tpath   -- ダウンロード対象のADLSGen2URL。\r\n",
							"\theader -- ヘッダー。 (default True)\r\n",
							"\t\"\"\"\r\n",
							"\ttry:\r\n",
							"\t\tlogger.info(\"parquet形式ファイルデータの取得を開始します。[path:{} header:{}]\".format(path, header))\r\n",
							"\t\t# parquet形式ファイルの取得\r\n",
							"\t\tdf = spark.read\\\r\n",
							"\t\t\t.parquet(\r\n",
							"\t\t\t\tpath, \r\n",
							"\t\t\t\theader=header)\r\n",
							"\t\tlogger.info(\"parquet形式ファイルデータデータの取得が完了しました。[データ:{}]\".format(df))\r\n",
							"\t\treturn df\r\n",
							"\texcept AnalysisException as e:\r\n",
							"\t\tlogger.error(\"パスにファイルが存在しません。[パス:{} e:{}]\".format(path, e))\r\n",
							"\t\traise\r\n",
							"\texcept Exception as e:\r\n",
							"\t\tlogger.error(\"データ取得に失敗しました。[パス:{} e:{}]\".format(path, e))\r\n",
							"\t\traise\r\n",
							"\r\n",
							"# def spark_read_text(path):\r\n",
							"# \t\"\"\"\r\n",
							"# \tSparkを使ってADLSGen2上のテキストファイルを読み込み、\r\n",
							"# \tSparkDataFrameにして返す。\r\n",
							"\r\n",
							"# \tKeyword arguments:\r\n",
							"# \tlogger -- ロガー\r\n",
							"# \tpath   -- ダウンロード対象のADLSGen2URL。\r\n",
							"# \t\"\"\"\r\n",
							"# \ttry:\r\n",
							"# \t\tlogger.info(\"テキスト形式ファイルデータの取得を開始します。[path:{}]\".format(path))\r\n",
							"# \t\t# テキスト形式ファイルの取得\r\n",
							"# \t\tdf = spark.read.text(path)\r\n",
							"# \t\tlogger.info(\"テキスト形式ファイルデータの取得が完了しました。[データ:{}]\".format(df))\r\n",
							"# \t\treturn df\r\n",
							"# \texcept AnalysisException as e:\r\n",
							"# \t\tlogger.error(\"パスにファイルが存在しません。[パス:{} e:{}]\".format(path, e))\r\n",
							"# \t\traise\r\n",
							"# \texcept Exception as e:\r\n",
							"# \t\tlogger.error(\"データ取得に失敗しました。[パス:{} e:{}]\".format(path, e))\r\n",
							"# \t\traise\r\n",
							"\r\n",
							"# def spark_write(df_upload, path, format_type=\"parquet\", mode_type=\"overwrite\", coalesce=1):\r\n",
							"# \t\"\"\"\r\n",
							"# \tSparkを使ってADLSGen2上にSparkDataFrame情報をファイル出力します。\r\n",
							"\r\n",
							"# \tKeyword arguments:\r\n",
							"# \tlogger      -- ロガー\r\n",
							"# \tdf_upload   -- アップロード対象データのSparkDataFrame\r\n",
							"# \tpath        -- ダウンロード対象のADLSGen2URL\r\n",
							"# \tformat_type -- ファイル形式 (default parquet)\r\n",
							"# \tmode_type   -- 書き込みモード (default overwrite)\r\n",
							"# \tcoalesce    -- 出力ファイル数 (default 1)\r\n",
							"# \t\"\"\"\r\n",
							"# \ttry:\r\n",
							"# \t\tlogger.info(\"ファイルをアップロードします。[df_upload:{} path:{} format_type:{} mode_type:{} coalesce:{}]\" \\\r\n",
							"# \t\t\t.format(df_upload, path, format_type, mode_type, coalesce))\r\n",
							"# \t\tdf_upload.coalesce(coalesce).write. \\\r\n",
							"# \t\t\tformat(format_type).mode(mode_type).save(path, header = 'true')\r\n",
							"# \texcept Exception as e:\r\n",
							"# \t\tlogger.error(\"ファイルアップロードに失敗しました。[パス:{} e:{}]\".format(path, e))\r\n",
							"# \t\traise\r\n",
							"\r\n",
							"# def check_date(trade_date):\r\n",
							"# \t\"\"\"\r\n",
							"# \t取引日がyyyyMMdd形式であるか確認する。\r\n",
							"# \t形式不備があれば、エラー終了する。\r\n",
							"\r\n",
							"# \tKeyword arguments:\r\n",
							"# \ttrade_date -- 取引日。\r\n",
							"# \t\"\"\"\r\n",
							"# \ttry:\r\n",
							"# \t\t# yyyyMMdd形式想定でdatetimeにcastする\r\n",
							"# \t\tdatetime.datetime.strptime(trade_date,\"%Y%m%d\")\r\n",
							"# \texcept ValueError:\r\n",
							"# \t\tlogger.error(\"取引日がyyyyMMdd形式ではありません。[取引日:{}]\".format(trade_date))\r\n",
							"# \t\traise\r\n",
							"\r\n",
							"def main():\r\n",
							"\t\"\"\"\r\n",
							"\tデータ変換処理を実行する。\r\n",
							"\t\"\"\"\r\n",
							"\r\n",
							"\t# -----------------------------------------------------\r\n",
							"\t# \r\n",
							"\t# Sparkジョブの引数の取得\r\n",
							"\t# \r\n",
							"\t# -----------------------------------------------------\r\n",
							"\tlogger.info(\"# -----------------------------------------------------\")\r\n",
							"\tlogger.info(\"# \")\r\n",
							"\tlogger.info(\"# Sparkジョブの引数の設定\")\r\n",
							"\tlogger.info(\"# \")\r\n",
							"\tlogger.info(\"# -----------------------------------------------------\")\r\n",
							"\r\n",
							"\t# 引数の取得\r\n",
							"\tproject         = 'MONITORING'\r\n",
							"\ttrade_date      = '20230821'\r\n",
							"\tsection         = 'daily_monitoring'\r\n",
							"\tdatamart_table  = 'OAPC_MANAGEMENT_INFO'\r\n",
							"\r\n",
							"\tinputdata_file_path            = 'wasbs://pythontest@testsynpaseblobstorage.blob.core.windows.net/test/EXCEL.OAPC_MANAGEMENT_INFO.txt'\r\n",
							"    # inputdata_file_path              = 'wasbs://pythontest@testsynpaseblobstorage.blob.core.windows.net/test/EXCEL.OAPC_MANAGEMENT_INFO.txt'\r\n",
							"    # inputdata_file_path              ='wasbs://pythontest@testsynpaseblobstorage.blob.core.windows.net/test/EXCEL.OAPC_MANAGEMENT_INFO.txt'\r\n",
							"\t# outputdata_file_path           = 'abfss://datamart@stragegen2prdje001.dfs.core.windows.net/MONITORING/OAPC_MANAGEMENT_INFO/2023/08/21'\r\n",
							"\tservicelink_name               = 'AzureBlobStorage11'\r\n",
							"    # servicelink_name               = 'AzureBlobStorage11'\r\n",
							"\r\n",
							"\t# -----------------------------------------------------\r\n",
							"\t# \r\n",
							"\t# BLOB向けSASトークンの作成\r\n",
							"\t# \r\n",
							"\t# -----------------------------------------------------\r\n",
							"\tlogger.info(\"# -----------------------------------------------------\")\r\n",
							"\tlogger.info(\"# \")\r\n",
							"\tlogger.info(\"# BLOB向けSASトークンの作成\")\r\n",
							"\tlogger.info(\"# \")\r\n",
							"\tlogger.info(\"# -----------------------------------------------------\")\r\n",
							"\t# 出力先のパスからストレージ名とコンテナ名を抽出して、SASトークンを取得する\r\n",
							"\tblob_account_name = 'testsynpaseblobstorage' # replace with your blob name\r\n",
							"\tblob_container_name = 'pythontest' # replace with your container name\r\n",
							"\tblob_relative_path = 'test/EXCEL.OAPC_MANAGEMENT_INFO.txt' # replace with your relative folder path\r\n",
							"\tlinked_service_name = 'AzureBlobStorage11' # replace with your linked service name\r\n",
							"\tblob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name)\r\n",
							"\t# wasb_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\r\n",
							"    # spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token)\r\n",
							"    # spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token)\r\n",
							"    # print('Remote blob path: ' + wasb_path)\r\n",
							"\r\n",
							"    # spark.conf.set(output_storage_path, blob_sas_token)\r\n",
							"    # blob_account_name = 'testsynpaseblobstorage' # replace with your blob name\r\n",
							"    # blob_container_name = 'pythontest' # replace with your container name\r\n",
							"    # blob_relative_path = 'test/EXCEL.OAPC_MANAGEMENT_INFO.txt' # replace with your relative folder path\r\n",
							"    # linked_service_name = 'AzureBlobStorage11' # replace with your linked service name\r\n",
							"\r\n",
							"    # blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name)\r\n",
							"\r\n",
							"    # Allow SPARK to access from Blob remotely\r\n",
							"    # wasb_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\r\n",
							"    # spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token)\r\n",
							"    # print('Remote blob path: ' + wasb_path)\r\n",
							"\r\n",
							"    # wasb_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\r\n",
							"\r\n",
							"    \r\n",
							"\r\n",
							"\t# -----------------------------------------------------\r\n",
							"\t# 各種ファイルのダウンロード\r\n",
							"\t# -----------------------------------------------------\r\n",
							"\t# ローデータのダウンロード\r\n",
							"    # spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token)\r\n",
							"\tspark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token)\r\n",
							"    \r\n",
							"    # df_input_data = spark_read_csv(inputdata_file_path, ',')\r\n",
							"\r\n",
							"\tdf_input_data = spark_read_csv(inputdata_file_path, ',')\r\n",
							"\twindow = Window.orderBy(F.col('monotonically_increasing_id'))\r\n",
							"\tdf_input_data = df_input_data.withColumn(\"monotonically_increasing_id\", F.monotonically_increasing_id())\r\n",
							"\tdf_input_data = df_input_data.withColumn('id', F.row_number().over(window)).drop(\"monotonically_increasing_id\")\r\n",
							"\tdata_diff_cnt = 0\r\n",
							"\tdata_diff_sum = 0.0\r\n",
							"\tdiff_col_name = \"\"\r\n",
							"\tdiff_col_name_warn = \"\"\r\n",
							"\tdata_diff_result = \"\"\r\n",
							"\r\n",
							"\tif (df_input_data.count() > 0):\r\n",
							"\t\t# logger.info(\"インプットとアウトプットの差分レコード数・合計値の計算を実施します。\")\r\n",
							"\t\t\r\n",
							"\t\t# # インプットとアウトプットの差分レコード数算出\r\n",
							"\t\t# data_diff_cnt = (df_input_data.select(df_output_data.columns).sort(\"id\")).subtract(df_output_data.select(df_output_data.columns).sort(\"id\")).count()\r\n",
							"\r\n",
							"\t\t# インプットとアウトプットの値の差分の合計算出\r\n",
							"\r\n",
							"\t\t# エラー発生箇所\r\n",
							"\t\tdf_input_data.show()\r\n",
							"\t\tdf_input_data.withColumnRenamed(\"Python3.8\",\"Python3.8_output\").show()\r\n",
							"\t\tprint(\"=======001==========\")\r\n",
							"\t\t# print([F.col(c.replace('`', '\"')).alias(c + \"_input\") for c in df_input_data.columns])\r\n",
							"\t\tdf_input_data_join = df_input_data.select([F.col(c).alias(\"`\"+c+\"`_output\") for c in df_input_data.columns])\r\n",
							"\t\t# print(df_input_data_join)\r\n",
							"\t\t# print(\"=======002==========\")\r\n",
							"\t\t# print([ F.col(c).alias(c + \"_output\") for c in df_input_data.columns])\r\n",
							"\t\t# df_input_data_join  = df_input_data.select([ F.col(c).alias(c + \"_output\") for c in df_input_data.columns])\r\n",
							"\t\t# print(\"========003=========\")\r\n",
							"\t\t# columns = [ F.col().alias(c + \"_output\") for c in df_input_data.columns]\r\n",
							"\t\t# df_input_data_join  = df_input_data.select([F.col(c.alias(c + \"_output\")) for c in df_input_data.columns])\r\n",
							"\t\t# print(\"=================\")\r\n",
							"\r\n",
							"\t\t# df_output_data_join = df_output_data.select([F.col(c.replace('`', '')).alias(c + \"_output\") for c in df_output_data.columns])\r\n",
							"\t\t# df_join = df_input_data_join.join(df_output_data_join, df_input_data_join.id_input == df_output_data_join.id_output, \"left\")\r\n",
							"\r\n",
							"\t# \t# オンプレとDMTでのレコードの差分値を計算する\r\n",
							"\t# \tfor row in df_output_data.dtypes:\r\n",
							"\t# \t\tif row[1] == 'string':\r\n",
							"\t# \t\t\t# string型の場合\r\n",
							"\t# \t\t\tcnt = (df_input_data.select(row[0])).subtract(df_output_data.select(row[0])).count()\r\n",
							"\t# \t\t\tdata_diff_sum = data_diff_sum + cnt\r\n",
							"\t# \t\t\tif cnt > 0:\r\n",
							"\t# \t\t\t\t# 誤差0以外はNGとする\r\n",
							"\t# \t\t\t\tdiff_col_name = diff_col_name + \",\" + row[0]\r\n",
							"\t# \t\telse:\r\n",
							"\t# \t\t\tdf_temp = df_join.fillna(0).withColumn(\"diff\", F.col(row[0].replace('`', '') + \"_input\") - F.col(row[0].replace('`', '') + \"_output\"))\r\n",
							"\t# \t\t\tcol_sum = df_temp.select(F.abs(F.col(\"diff\"))).groupBy().sum().collect()[0][0]\r\n",
							"\r\n",
							"\t# \t\t\tif col_sum is None:\r\n",
							"\t# \t\t\t\tcol_sum = float(df_temp.fillna(0).fillna(\"0\").withColumn(row[0].replace('`', '') + \"_input\",F.col(row[0].replace('`', '') + \"_input\").cast('decimal')).withColumn(\"diff\", F.col(row[0].replace('`', '') + \"_input\") - F.col(row[0].replace('`', '') + \"_output\")).select(F.abs(F.col(\"diff\"))).groupBy().sum().collect()[0][0])\r\n",
							"\t\t\t\t\t\r\n",
							"\t# \t\t\tdata_diff_sum = data_diff_sum + col_sum\r\n",
							"\t\t\t\t\r\n",
							"\t# \t\t\tif re.match('decimal\\([0-9]+,[0-9]+\\)', row[1]):\r\n",
							"\t# \t\t\t\t# decimal(m,n)型の場合\t\t\t\t\t\r\n",
							"\t# \t\t\t\tif Decimal(col_sum) >= Decimal(Decimal(\"0.1\")**int(row[1].replace(\"(\",\",\").replace(\")\",\",\").split(',')[2])):\r\n",
							"\t# \t\t\t\t\t# 誤差が1e-(DMT側で保持する小数点以下桁数)以上であればNGとする\r\n",
							"\t# \t\t\t\t\tdiff_col_name = diff_col_name + \",\" + row[0]\r\n",
							"\t# \t\t\t\telif Decimal(col_sum) > Decimal(\"0\"):\r\n",
							"\t# \t\t\t\t\t# 0超1e-(DMT側で保持する小数点以下桁数)未満であればWARNとする\t\t\t\t\t\t\t\r\n",
							"\t# \t\t\t\t\tdiff_col_name_warn = diff_col_name_warn + \",\" + row[0]\t\t\t\t\t\r\n",
							"\t# \t\t\telse:\r\n",
							"\t# \t\t\t\t# decimal(m,n)以外の数値型(decimal(n)も含む)の場合\r\n",
							"\t# \t\t\t\tif col_sum > 0:\r\n",
							"\t# \t\t\t\t\t# 誤差が1以上であればNGとする\r\n",
							"\t# \t\t\t\t\tdiff_col_name = diff_col_name + \",\" + row[0]\r\n",
							"\t\t\t\t\r\n",
							"\t# \tif diff_col_name:\r\n",
							"\t# \t\tdata_diff_result = \"NG\"\r\n",
							"\t# \telif diff_col_name_warn:\r\n",
							"\t# \t\tdata_diff_result = \"WARN\"\r\n",
							"\t# \telse:\r\n",
							"\t# \t\tdata_diff_result = \"OK\"\r\n",
							"\r\n",
							"\t# \ttest_result_list = []\r\n",
							"\t# \ttest_result_dict = {\r\n",
							"\t# \t\t\t\"key\"              \t: project + \"_\" + trade_date + \"_\" + datamart_name,\r\n",
							"\t# \t\t\t\"project\"          \t: project,\r\n",
							"\t# \t\t\t\"trade_date\"       \t: trade_date,\r\n",
							"\t# \t\t\t\"datamart_name\"    \t: datamart_name,\r\n",
							"\t# \t\t\t\"rowcnt_input\"     \t: df_input_data.count(),\r\n",
							"\t# \t\t\t\"rowcnt_output\"    \t: df_output_data.count(),\r\n",
							"\t# \t\t\t\"rowcnt_diff\"      \t: df_input_data.count()-df_output_data.count(),\r\n",
							"\t# \t\t\t\"colcnt_input\"     \t: len(df_input_data.columns),\r\n",
							"\t# \t\t\t\"colcnt_output\"    \t: len(df_output_data.columns),\r\n",
							"\t# \t\t\t\"colcnt_diff\"      \t: len(df_input_data.columns)-len(df_output_data.columns),\r\n",
							"\t# \t\t\t\"data_diff_rowcnt\" \t: data_diff_cnt,\r\n",
							"\t# \t\t\t\"data_diff_sum\"    \t: data_diff_sum,\r\n",
							"\t# \t\t\t\"diff_col_name\"    \t: diff_col_name,\r\n",
							"\t# \t\t\t\"diff_col_name_warn\": diff_col_name_warn,\r\n",
							"\t# \t\t\t\"data_diff_result\"\t: data_diff_result \r\n",
							"\t# \t\t}\r\n",
							"\t# \ttest_result_list.append(test_result_dict)\r\n",
							"\t# \tlogger.info(\"テストを実施しました。[test_result_dict:{}]\".format(test_result_list))\r\n",
							"\r\n",
							"\t# \t# カウントをインクリメント\r\n",
							"\t# \tdatamart_count += 1\r\n",
							"\r\n",
							"\t# \t# 結果の暫定出力\r\n",
							"\t# \tdf_test_result = spark.createDataFrame(test_result_list)\r\n",
							"\t# \tdf_test_result = df_test_result.select([\"key\", \"project\", \"trade_date\", \"datamart_name\", \"rowcnt_input\", \"rowcnt_output\", \"rowcnt_diff\", \"colcnt_input\", \"colcnt_output\", \"colcnt_diff\", \"data_diff_rowcnt\", \"data_diff_sum\", \"diff_col_name\", \"diff_col_name_warn\", \"data_diff_result\"])\r\n",
							"\t# \tdf_test_result.show()\r\n",
							"\t# \tspark_write(\r\n",
							"\t# \t\tdf_test_result, \r\n",
							"\t# \t\trename_path_project_datamart(rename_path_yyyymmdd(test_result_path), datamart_name),\r\n",
							"\t# \t\tformat_type=\"parquet\", \r\n",
							"\t# \t\tmode_type=\"overwrite\")\r\n",
							"\r\n",
							"\t# \t# テスト結果NGの場合はエラー終了する\r\n",
							"\t# \talert_target_sum = df_test_result.select(F.sum(F.col(\"rowcnt_diff\"))).groupBy().sum().collect()[0][0]\r\n",
							"\t# \tif alert_target_sum > 0 or diff_col_name:\r\n",
							"\t# \t\terror_count = error_count + 1\r\n",
							"\t# \t\terror_datamart_list.append({\"project\":project ,\"datamart_name\":datamart_name})\r\n",
							"\t\t\t\r\n",
							"\r\n",
							"\t# # テスト結果NGの場合はエラー終了する\r\n",
							"\t# if error_count > 0:\r\n",
							"\t# \traise Exception(\"テスト結果NGでした。[プロジェクト-データマート:{}\".format(error_datamart_list))\r\n",
							"\r\n",
							"\r\n",
							"try:\r\n",
							"\t# メイン処理\r\n",
							"\tmain()\r\n",
							"except Exception as e:\r\n",
							"\tlogger.error(\"ETL実行中にエラーが発生しました。[e:{}]\".format(e))\r\n",
							"\traise\r\n",
							""
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 15')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "test"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "4fe49784-9853-4e0a-bf34-0e532537f018"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"log4jLogger = spark.sparkContext._jvm.org.apache.log4j  \r\n",
							"\r\n",
							"customLogs = log4jLogger.LogManager.getLogger(\"CustomLogs\")  \r\n",
							"\r\n",
							" \r\n",
							"\r\n",
							"customLogs.info(\"TestingA INFO Logs\") \r\n",
							"\r\n",
							"customLogs.warn(\"TestingA WARN Logs\") \r\n",
							"\r\n",
							"customLogs.error(\"TestingA ERROR Logs\") \r\n",
							"\r\n",
							"print(\"This message will be in stdout instead of log4j\") "
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 16')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "test"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "36f61dcb-4af9-444a-aea8-c838d5b7cbae"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from notebookutils import mssparkutils \r\n",
							"\r\n",
							"mssparkutils.fs.mount( \r\n",
							"    \"abfss://log@testfansynapseadlsgen2.dfs.core.windows.net\", \r\n",
							"    \"/test\", \r\n",
							"    {\"LinkedService\":\"AzureDataLakeStorage1\"} \r\n",
							") \r\n",
							""
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"myid=mssparkutils.env.getJobId()\r\n",
							"myid"
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 17')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "test"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "c1afb954-5bce-4434-9b2a-67da6885d59f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.types import *\r\n",
							"\r\n",
							"account_name = \"testfansynapseadlsgen2\"\r\n",
							"container_name = \"headertest\"\r\n",
							"relative_path = \"test\"\r\n",
							"\r\n",
							"adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path)\r\n",
							"\r\n",
							"dataframe = spark.read.option('header','false').text(adls_path+'/incoming2.dat')\r\n",
							"\r\n",
							"result = []\r\n",
							"record_flag = True\r\n",
							"appendtemp = ''\r\n",
							"\r\n",
							"for row in dataframe.rdd.collect():\r\n",
							"    # print(\"============\")\r\n",
							"    # print(row)\r\n",
							"    # print(row.value)\r\n",
							"    # print(type(row))\r\n",
							"    # print(type(row.value))\r\n",
							"    # print(len(row.value))\r\n",
							"    \r\n",
							"    if len(row.value) > 46:\r\n",
							"        if appendtemp:\r\n",
							"            result.append(appendtemp + \"                     00000\")\r\n",
							"            record_flag = True\r\n",
							"\r\n",
							"        # spilt into 4 parts\r\n",
							"        temp = [row.value[i:i+80] for i in range(0,len(row.value),80)]\r\n",
							"        for item in temp:\r\n",
							"            result.append(item)\r\n",
							"    else:\r\n",
							"        if record_flag: \r\n",
							"            appendtemp = row.value\r\n",
							"            record_flag = False\r\n",
							"        else:\r\n",
							"            result.append(appendtemp+row.value[5:])\r\n",
							"            appendtemp = ''\r\n",
							"            record_flag = True\r\n",
							"\r\n",
							"if appendtemp:\r\n",
							"    result.append(appendtemp + \"                     00000\")\r\n",
							"\r\n",
							"\r\n",
							"print(result)\r\n",
							"\r\n",
							"\r\n",
							"date_df= spark.createDataFrame([(l,) for l in result])\r\n",
							"\r\n",
							"date_df.show()\r\n",
							"\r\n",
							"date_df.coalesce(1).write.format(\"text\").option(\"header\", \"false\").mode(\"append\").save(adls_path+\"/output.txt\")"
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 18')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "test"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "327f26b7-f673-47b4-810d-33b3428d7ade"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from notebookutils import mssparkutils\r\n",
							"\r\n",
							"# https://testfansynapseadlsgen2.blob.core.windows.net/log/sample.csv\r\n",
							"myfile = 'abfss://log@testfansynapseadlsgen2.dfs.core.windows.net/sample.csv'\r\n",
							"mssparkutils.fs.ls(myfile)"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from notebookutils import mssparkutils\r\n",
							"\r\n",
							"# https://testfansynapseadlsgen2.blob.core.windows.net/log/sample.csv\r\n",
							"myfile = 'abfss://log@testfansynapseadlsgen2.dfs.core.windows.net/'\r\n",
							"mssparkutils.fs.ls(myfile)"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import logging\r\n",
							"\r\n",
							"logger = logging.getLogger()\r\n",
							"logger.setLevel(logging.DEBUG)\r\n",
							"\r\n",
							"formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\r\n",
							"\r\n",
							"#ファイルへ出力するハンドラーを定義\r\n",
							"fh = logging.FileHandler(filename=myfile, encoding='utf-8')\r\n",
							"fh.setLevel(logging.DEBUG)\r\n",
							"fh.setFormatter(formatter)\r\n",
							"#rootロガーにハンドラーを登録する\r\n",
							"logger.addHandler(fh)\r\n",
							"\r\n",
							"logger.debug(\"ログに出力\")\r\n",
							"logger.info(\"ログに出力\")\r\n",
							"logger.warning(\"ログに出力\")\r\n",
							"logger.error(\"ログに出力\")"
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 19')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "test"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "testDotNt",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "6e9e0e07-eb9f-460a-bada-687be416e399"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2/resourceGroups/test-synapse1/providers/Microsoft.Synapse/workspaces/test-synapse1-fan/bigDataPools/testDotNt",
						"name": "testDotNt",
						"type": "Spark",
						"endpoint": "https://test-synapse1-fan.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/testDotNt",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%%configure -f\r\n",
							"{\r\n",
							"    \"driverMemory\": \"33g\",\r\n",
							"    \"driverCores\": 1,\r\n",
							"    \"executorMemory\": \"33g\",\r\n",
							"    \"executorCores\": 1,\r\n",
							"    \"numExecutors\" : 1\r\n",
							"}"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import random\r\n",
							"\r\n",
							"def inside(p):\r\n",
							"    x, y = random.random(), random.random()\r\n",
							"    return x*x + y*y < 1\r\n",
							"\r\n",
							"count = sc.parallelize(range(0, 100000)) \\\r\n",
							"             .filter(inside).count()\r\n",
							"print(\"Pi is roughly %f\" % (4.0 * count / 100000))\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 21')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "test"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "testsparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "cd5aacf9-a6b3-496c-9d7f-6b944e4d338c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2/resourceGroups/test-synapse1/providers/Microsoft.Synapse/workspaces/test-synapse1-fan/bigDataPools/testsparkpool",
						"name": "testsparkpool",
						"type": "Spark",
						"endpoint": "https://test-synapse1-fan.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/testsparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%%configure -f\r\n",
							"{\r\n",
							"    \"conf\":{\r\n",
							"        \"spark.kryoserializer.buffer.max\":\"1280\",\r\n",
							"        \"spark.driver.maxResultSize\":\"999g\",\r\n",
							"        \"spark.sql.execution.arrow.pyspark.enabled\":\"True\",\r\n",
							"        \"spark.sql.analyzer.failAmbiguousSelfJoin\":\"False\"\r\n",
							"    }\r\n",
							"}"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"import random\r\n",
							"\r\n",
							"def inside(p):\r\n",
							"    x, y = random.random(), random.random()\r\n",
							"    return x*x + y*y < 1\r\n",
							"\r\n",
							"count = sc.parallelize(range(0, 100000)) \\\r\n",
							"             .filter(inside).count()\r\n",
							"print(\"Pi is roughly %f\" % (4.0 * count / 100000))"
						],
						"outputs": [],
						"execution_count": 6
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 22')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "testDotNt",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "1d7c1188-11a7-457b-abd3-b25dc5b22ea4"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2/resourceGroups/test-synapse1/providers/Microsoft.Synapse/workspaces/test-synapse1-fan/bigDataPools/testDotNt",
						"name": "testDotNt",
						"type": "Spark",
						"endpoint": "https://test-synapse1-fan.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/testDotNt",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"# abfss://test1020@yikeren1020.dfs.core.windows.net//fan/test.csv\r\n",
							"import pandas as pd\r\n",
							"df = pd.read_csv(\"abfss://test1020@yikeren1020.dfs.core.windows.net//fan/test.csv\", \\\r\n",
							"    storage_options = {\"sas_token\" : \"<SAS_TOKEN>\"})\r\n",
							"print(df)\r\n",
							""
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 23')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "testconf",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "4693c6d0-634f-46bb-bd48-9ed21902cde1"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2/resourceGroups/test-synapse1/providers/Microsoft.Synapse/workspaces/test-synapse1-fan/bigDataPools/testconf",
						"name": "testconf",
						"type": "Spark",
						"endpoint": "https://test-synapse1-fan.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/testconf",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"logger = sc._jvm.org.apache.log4j.LogManager.getLogger(\"com.contoso.PythonLoggerExample\")\r\n",
							"logger.info(\"info message\")\r\n",
							"logger.warn(\"warn message\")\r\n",
							"logger.error(\"error message\")"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"print(\"hello world\")"
						],
						"outputs": [],
						"execution_count": 4
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 24')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "59e6135a-396c-454a-b3d4-428a32c2cc90"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 6')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "AccessADLSGen2"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "testDotNt",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b9abf222-362c-4535-9b3f-2a63b46ebf07"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2/resourceGroups/test-synapse1/providers/Microsoft.Synapse/workspaces/test-synapse1-fan/bigDataPools/testDotNt",
						"name": "testDotNt",
						"type": "Spark",
						"endpoint": "https://test-synapse1-fan.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/testDotNt",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from azure.identity import DefaultAzureCredential\r\n",
							"from azure.storage.filedatalake import DataLakeServiceClient\r\n",
							"\r\n",
							"credential = DefaultAzureCredential()\r\n",
							"service_client = DataLakeServiceClient(account_url=\"{}://{}.dfs.core.windows.net\".format(\"https\", \"fanxintestadls\"), credential=credential)\r\n",
							"file_system_client = service_client.get_file_system_client(file_system=\"test\")\r\n",
							"\r\n",
							"paths = file_system_client.get_paths()\r\n",
							"for path in paths:\r\n",
							"    print(path.name)"
						],
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 7')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "AccessDatabase"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "testmpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9a730f33-aa74-4e0c-a883-9179e941bcae"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2/resourceGroups/test-synapse1/providers/Microsoft.Synapse/workspaces/test-synapse1-fan/bigDataPools/testmpool",
						"name": "testmpool",
						"type": "Spark",
						"endpoint": "https://test-synapse1-fan.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/testmpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import pyodbc\r\n",
							"pyodbc.drivers()\r\n",
							""
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pyodbc\r\n",
							"server = 'test-synapse1-fan-ondemand.sql.azuresynapse.net'\r\n",
							"database = 'test'\r\n",
							"username = 'username'\r\n",
							"password = 'password'\r\n",
							"driver= 'ODBC Driver 17 for SQL Server'\r\n",
							"with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+server+';PORT=1433;DATABASE='+database+';UID='+username+';PWD='+ password) as conn:\r\n",
							"    with conn.cursor() as cursor:\r\n",
							"        cursor.execute(\"SELECT TOP 3 name, collation_name FROM sys.databases\")\r\n",
							"        row = cursor.fetchone()\r\n",
							"        while row:\r\n",
							"            print (str(row[0]) + \" \" + str(row[1]))\r\n",
							"            row = cursor.fetchone()\r\n",
							""
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"cnxn = pyodbc.connect(\"Driver=ODBC Driver 17 for SQL Server;Server=tcp:test-synapse1-fan-ondemand.sql.azuresynapse.net,1433;Database=test;Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;Authentication=ActiveDirectoryIntegrated\")"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azure.identity import DefaultAzureCredential\r\n",
							"from azure.synapse.artifacts import ArtifactClient\r\n",
							"from azure.synapse.sql import SqlManagementClient\r\n",
							"from azure.core.exceptions import HttpResponseError\r\n",
							"\r\n",
							"credential = DefaultAzureCredential()\r\n",
							"artifact_client = ArtifactClient(endpoint='https://test-synapse1-fan.dev.azuresynapse.net', credential=credential)\r\n",
							"sql_client = SqlManagementClient(endpoint='https://test-synapse1-fan.dev.azuresynapse.net', credential=credential)\r\n",
							"try:\r\n",
							"    sql_pool = sql_client.sql_pools.get('SynapseDedicatedSQLPool', 'test-synapse1')\r\n",
							"    print(f\"Connected to SQL Pool '{sql_pool.name}'\")\r\n",
							"except HttpResponseError as error:\r\n",
							"    print(error)"
						],
						"outputs": [],
						"execution_count": 18
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 8')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "AccessDatabase"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "testsparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "f4f78040-d2f4-4b29-a2bd-e146ec950170"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2/resourceGroups/test-synapse1/providers/Microsoft.Synapse/workspaces/test-synapse1-fan/bigDataPools/testsparkpool",
						"name": "testsparkpool",
						"type": "Spark",
						"endpoint": "https://test-synapse1-fan.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/testsparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import pyodbc \r\n",
							"\r\n",
							"server = 'test-synapse1-fan-ondemand.sql.azuresynapse.net' \r\n",
							"database = 'test' \r\n",
							"authentication = 'ActiveDirectoryIntegrated'\r\n",
							"kpi_server_connection = pyodbc.connect('DRIVER=ODBC Driver 17 for SQL Server;SERVER='+server+';DATABASE='+database+';Authentication='+authentication+';TrustServerCertificate='+ 'no')\r\n",
							"\r\n",
							"query_string = '''\r\n",
							"    SELECT TOP 3 name, collation_name FROM sys.databases\r\n",
							"    '''\r\n",
							"df = pd.read_sql(query_string, kpi_server_connection)\r\n",
							"# df"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"cnxn = pyodbc.connect(\"Driver={ODBC Driver 17 for SQL Server};Server=tcp:khawajaserver1.database.windows.net,1433;Database=KhawajaDB1;Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;Authentication=ActiveDirectoryIntegrated\")"
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 9')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "AccessDatabase"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "testsparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "816732a7-878e-4dad-ab32-5370ccc3469d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2/resourceGroups/test-synapse1/providers/Microsoft.Synapse/workspaces/test-synapse1-fan/bigDataPools/testsparkpool",
						"name": "testsparkpool",
						"type": "Spark",
						"endpoint": "https://test-synapse1-fan.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/testsparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import col \r\n",
							"\r\n",
							"# Read from existing internal table\r\n",
							"# test-synapse1-fan.sql.azuresynapse.net\r\n",
							"# https://testsynapse12.dfs.core.windows.net\r\n",
							"dfToReadFromTable = (spark.read.option(Constants.SERVER, \"test-synapse1-fan.sql.azuresynapse.net\")\r\n",
							"    .option(Constants.TEMP_FOLDER, \"abfss://test@testsynapse12.dfs.core.windows.net/\")\r\n",
							"    # .option(Constants.USER, \"sqladminuser\")\r\n",
							"    # .option(Constants.PASSWORD, \"Pa$$w0rd1234\")\r\n",
							"    .synapsesql(\"SynapseDedicatedSQLPool.dbo.Persons\")\r\n",
							"    .select(\"*\")\r\n",
							"    .limit(3)) \r\n",
							"\r\n",
							"# Show contents of the dataframe\r\n",
							"dfToReadFromTable.show()\r\n",
							"\r\n",
							"print(dfToReadFromTable)\r\n",
							"print(\"==================\")\r\n",
							"dfToReadFromTable.printSchema()\r\n",
							"\r\n",
							"\r\n",
							"print(dfToReadFromTable.limit(10))"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import col \r\n",
							"\r\n",
							"# Read from existing internal table\r\n",
							"# test-synapse1-fan.sql.azuresynapse.net\r\n",
							"# https://testsynapse12.dfs.core.windows.net\r\n",
							"dfToReadFromTable = (spark.read.option(Constants.SERVER, \"test-synapse1-fan-ondemand.sql.azuresynapse.net\")\r\n",
							"    .option(Constants.TEMP_FOLDER, \"abfss://test@testsynapse12.dfs.core.windows.net/\")\r\n",
							"    .synapsesql(\"test123.sys.databases\")\r\n",
							"    .select(\"name\")\r\n",
							"    .limit(3)) \r\n",
							"\r\n",
							"# Show contents of the dataframe\r\n",
							"dfToReadFromTable.show()\r\n",
							"\r\n",
							"print(dfToReadFromTable)\r\n",
							"print(\"==================\")\r\n",
							"dfToReadFromTable.printSchema()\r\n",
							"\r\n",
							"\r\n",
							"print(dfToReadFromTable.limit(10))"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pyodbc\r\n",
							"server = 'test-synapse1-fan-ondemand.sql.azuresynapse.net'\r\n",
							"database = 'test'\r\n",
							"username = 'username'\r\n",
							"password = 'password'\r\n",
							"driver= 'ODBC Driver 17 for SQL Server'\r\n",
							"with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+server+';PORT=1433;DATABASE='+database+';UID='+username+';PWD='+ password) as conn:\r\n",
							"    with conn.cursor() as cursor:\r\n",
							"        cursor.execute(\"SELECT TOP 3 name, collation_name FROM sys.databases\")\r\n",
							"        row = cursor.fetchone()\r\n",
							"        while row:\r\n",
							"            print (str(row[0]) + \" \" + str(row[1]))\r\n",
							"            row = cursor.fetchone()"
						],
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SparkCSV')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "AccessADLSGen2"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "testDotNt",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "8981b317-f1cf-4c65-b576-181d59014211"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1",
						"state": {
							"c23db1ac-03e1-4d4f-8de7-687349a05ed9": {
								"type": "Synapse.DataFrame",
								"sync_state": {
									"table": {
										"rows": [
											{
												"0": "1",
												"1": "apple"
											},
											{
												"0": "2",
												"1": "banana"
											},
											{
												"0": "3",
												"1": "orange"
											}
										],
										"schema": [
											{
												"key": "0",
												"name": "_c0",
												"type": "string"
											},
											{
												"key": "1",
												"name": "_c1",
												"type": "string"
											}
										],
										"truncated": false
									},
									"isSummary": false,
									"language": "scala"
								},
								"persist_state": {
									"view": {
										"type": "details",
										"chartOptions": {
											"chartType": "bar",
											"aggregationType": "count",
											"categoryFieldKeys": [
												"0"
											],
											"seriesFieldKeys": [
												"0"
											],
											"isStacked": false
										}
									}
								}
							}
						}
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2/resourceGroups/test-synapse1/providers/Microsoft.Synapse/workspaces/test-synapse1-fan/bigDataPools/testDotNt",
						"name": "testDotNt",
						"type": "Spark",
						"endpoint": "https://test-synapse1-fan.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/testDotNt",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"df = spark.read.load('wasbs://testcontainer@testsynpaseblobstorage.blob.core.windows.net/test/sample.csv', format='csv')\r\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": 8
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SparkStorage')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "AccessADLSGen2"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "testmpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a46cfbc0-a3c1-41e0-b7eb-a23dd7fd97b9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2/resourceGroups/test-synapse1/providers/Microsoft.Synapse/workspaces/test-synapse1-fan/bigDataPools/testmpool",
						"name": "testmpool",
						"type": "Spark",
						"endpoint": "https://test-synapse1-fan.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/testmpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"mssparkutils.credentials.help()"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"# Set the required configs\r\n",
							"# https://yikeren1020.blob.core.windows.net/test1020/fan/test.csv\r\n",
							"# https://yikeren1020.dfs.core.windows.net/\r\n",
							"source_full_storage_account_name = \"yikeren1020.dfs.core.windows.net\"\r\n",
							"spark.conf.set(f\"spark.storage.synapse.{source_full_storage_account_name}.linkedServiceName\", \"ADLSGen2Test2\")\r\n",
							"sc._jsc.hadoopConfiguration().set(f\"fs.azure.account.auth.type.{source_full_storage_account_name}\", \"SAS\")\r\n",
							"sc._jsc.hadoopConfiguration().set(f\"fs.azure.sas.token.provider.type.{source_full_storage_account_name}\", \"com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedSASProvider\")\r\n",
							"\r\n",
							"# Python code\r\n",
							"df = spark.read.csv('abfss://test1020@yikeren1020.dfs.core.windows.net/fan/test.csv')\r\n",
							"\r\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.credentials.getFullConnectionString(\"ADLSGen2Test\")"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Python code\r\n",
							"df = spark.read.csv('abfss://test@fantestmountstorage.dfs.core.windows.net/abc/sample.csv')\r\n",
							"\r\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Python code\r\n",
							"df = spark.read.csv('abfss://abc@datapolicy.dfs.core.windows.net/fanxintest/test.csv')\r\n",
							"\r\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 5
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TestA')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "testDotNt",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9fa34945-a2d4-48db-91b5-3caeea04b5e3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2/resourceGroups/test-synapse1/providers/Microsoft.Synapse/workspaces/test-synapse1-fan/bigDataPools/testDotNt",
						"name": "testDotNt",
						"type": "Spark",
						"endpoint": "https://test-synapse1-fan.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/testDotNt",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"hello world\")"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"source": [
							"from notebookutils import mssparkutils\r\n",
							"\r\n",
							"mssparkutils.notebook.run('testB')"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.notebook.run('testC')"
						],
						"outputs": [],
						"execution_count": 19
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/accessADLSgen2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "AccessADLSGen2"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "testsparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "a83fac2b-f531-4bec-80ae-f6a658a459a2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2/resourceGroups/test-synapse1/providers/Microsoft.Synapse/workspaces/test-synapse1-fan/bigDataPools/testsparkpool",
						"name": "testsparkpool",
						"type": "Spark",
						"endpoint": "https://test-synapse1-fan.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/testsparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"print(\"hello\")"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from notebookutils import mssparkutils\r\n",
							"file = 'abfss://test4synapse@testsynapseprivatefan.dfs.core.windows.net/test4synapse/sample.csv'\r\n",
							"mssparkutils.fs.ls('abfss://test4synapse@testsynapseprivatefan.dfs.core.windows.net/')\r\n",
							""
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"# https://testsynapseprivatefan.blob.core.windows.net/test4synapse/sample.csv\r\n",
							"# https://testsynapseprivatefan.blob.core.windows.net/\r\n",
							"\r\n",
							"# abfs[s]1://<file_system>2@<account_name>3.dfs.core.windows.net/<path>4/<file_name>5\r\n",
							"# https://testsynapseprivatefan.dfs.core.windows.net/\r\n",
							"df = spark.read.csv(\"abfss://test4synapse@testsynapseprivatefan.dfs.core.windows.net/sample.csv\")\r\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 1
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/accessBlobStorage')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "AccessADLSGen2"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "testmpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "fbeb4130-a772-40f9-9379-2bfdcaf5374a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2/resourceGroups/test-synapse1/providers/Microsoft.Synapse/workspaces/test-synapse1-fan/bigDataPools/testmpool",
						"name": "testmpool",
						"type": "Spark",
						"endpoint": "https://test-synapse1-fan.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/testmpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"\r\n",
							"# Azure storage access info\r\n",
							"blob_account_name = 'testsynpaseblobstorage' # replace with your blob name\r\n",
							"blob_container_name = 'testcontainer' # replace with your container name\r\n",
							"blob_relative_path = 'test/sample.csv' # replace with your relative folder path\r\n",
							"linked_service_name = 'AzureBlobStorage12' # replace with your linked service name\r\n",
							"\r\n",
							"blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name)\r\n",
							"\r\n",
							"# Allow SPARK to access from Blob remotely\r\n",
							"\r\n",
							"wasb_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\r\n",
							"\r\n",
							"spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token)\r\n",
							"print('Remote blob path: ' + wasb_path)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df = spark.read.load('wasbs://testcontainer@testsynpaseblobstorage.blob.core.windows.net/test/sample.csv', format='csv')"
						],
						"outputs": [],
						"execution_count": 5
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/accessBlobStorage2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "AccessADLSGen2"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "testsparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "9808a27b-82e1-4108-940d-ec236ce59023"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2/resourceGroups/test-synapse1/providers/Microsoft.Synapse/workspaces/test-synapse1-fan/bigDataPools/testsparkpool",
						"name": "testsparkpool",
						"type": "Spark",
						"endpoint": "https://test-synapse1-fan.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/testsparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%pip install fsspec\r\n",
							"%pip install adlfs"
						],
						"outputs": [],
						"execution_count": 91
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\r\n",
							"\r\n",
							"# Azure storage access info\r\n",
							"blob_account_name = 'testsynpaseblobstorage' # replace with your blob name\r\n",
							"blob_container_name = 'testcontainer' # replace with your container name\r\n",
							"blob_relative_path = 'test/sample.csv' # replace with your relative folder path\r\n",
							"linked_service_name = 'AzureBlobStorage11' # replace with your linked service name\r\n",
							"\r\n",
							"blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name)\r\n",
							"\r\n",
							"# Allow SPARK to access from Blob remotely\r\n",
							"\r\n",
							"wasb_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\r\n",
							"\r\n",
							"spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token)\r\n",
							"# print('Remote blob path: ' + wasb_path)\r\n",
							"\r\n",
							"import fsspec\r\n",
							"\r\n",
							"# 'abfss://log@testfansynapseadlsgen2.dfs.core.windows.net/'\r\n",
							"\r\n",
							"fs = fsspec.filesystem('abfss')\r\n",
							"print(fs.ls(\".\"))\r\n",
							"print(fs.ls(\"test\"))\r\n",
							"fs.ls(\"abfss://log@testfansynapseadlsgen2.dfs.core.windows.net/\", blob_sas_token)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 92
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import fsspec\r\n",
							"\r\n",
							"# 'abfss://log@testfansynapseadlsgen2.dfs.core.windows.net/'\r\n",
							"\r\n",
							"fs = fsspec.filesystem('abfss')\r\n",
							"print(fs.ls(\".\"))\r\n",
							"print(fs.ls(\"test\"))\r\n",
							"\r\n",
							"\r\n",
							"fsspec\r\n",
							"\r\n",
							"fs.ls(\"abfss://log@testfansynapseadlsgen2.dfs.core.windows.net/\")\r\n",
							"\r\n",
							"# fsspec.dircache()\r\n",
							"\r\n",
							"# print(fs.ls('abfss://log@testfansynapseadlsgen2.dfs.core.windows.net/'))\r\n",
							"\r\n",
							"from notebookutils import mssparkutils\r\n",
							"\r\n",
							"# https://testfansynapseadlsgen2.blob.core.windows.net/log/sample.csv\r\n",
							"myfile = 'abfss://log@testfansynapseadlsgen2.dfs.core.windows.net/'\r\n",
							"mssparkutils.fs.ls(myfile)\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import fsspec\r\n",
							"import pandas\r\n",
							"\r\n",
							"adls_account_name = '' #Provide exact ADLS account name\r\n",
							"sas_key = TokenLibrary.getConnectionString(<LinkedServiceName>)"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df = spark.read.load('wasbs://testcontainer@testsynpaseblobstorage.blob.core.windows.net/test/sample.csv', format='csv')"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.show()"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.select(\"_c1\").write.format(\"parquet\").save(\"wasbs://testcontainer@testsynpaseblobstorage.blob.core.windows.net/test/c1.parquet\")"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import fsspec\r\n",
							"storage_options={'account_name': 'testfansynapseadlsgen2', 'tenant_id': '72f988bf-86f1-41af-91ab-2d7cd011db47', 'client_id': '494ab310-3b17-4792-8b3d-31d1f15f0c52', 'client_secret': 'G6k8Q~syYxNBy4oVrarc.AZ-ZXpjhCOc4H6V-bWw'}\r\n",
							"fs = fsspec.filesystem(\"abfss\",**storage_options)\r\n",
							"print(fs.ls(\".\"))\r\n",
							"print(fs.ls(\"log\"))"
						],
						"outputs": [],
						"execution_count": 93
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import glob\r\n",
							"\r\n",
							"# wasbs://testcontainer@testsynpaseblobstorage.blob.core.windows.net/test/\r\n",
							"\r\n",
							"filename = \"wasbs://testcontainer@testsynpaseblobstorage.blob.core.windows.net/test/\"\r\n",
							"\r\n",
							"l = [pd.read_csv(filename) for filename in glob.glob(\"*.csv\")]\r\n",
							"print(l)\r\n",
							""
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/connectwithSQL')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "AccessDatabase"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "testDotNt",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d875d0e2-6be6-4638-9fd5-5acbd1ff5aa1"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2/resourceGroups/test-synapse1/providers/Microsoft.Synapse/workspaces/test-synapse1-fan/bigDataPools/testDotNt",
						"name": "testDotNt",
						"type": "Spark",
						"endpoint": "https://test-synapse1-fan.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/testDotNt",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"pip list"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import pyodbc\r\n",
							"pyodbc.drivers()"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"server = 'mysqlservercup.database.windows.net'\r\n",
							"database = 'mySampleDatabase'\r\n",
							"username = 'azureuser'\r\n",
							"password = 'Pa$$w0rd'\r\n",
							"\r\n",
							"cnxn = pyodbc.connect('DRIVER={ODBC Driver 18 for SQL Server};SERVER=' + server + ';DATABASE='+ database +';UID=' + username + ';PWD='+ password)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(cnxn)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"cur = cnxn.cursor()"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"cur.execute(\"select * from [dbo].[AAAA]\")"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"for row in cur:\r\n",
							"    print(row)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"cnxn.close()"
						],
						"outputs": [],
						"execution_count": 10
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/connectwithsynapsedatabase')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "AccessDatabase"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "testmpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0211df10-2f30-48b0-ab9a-ed86e33c955f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2/resourceGroups/test-synapse1/providers/Microsoft.Synapse/workspaces/test-synapse1-fan/bigDataPools/testmpool",
						"name": "testmpool",
						"type": "Spark",
						"endpoint": "https://test-synapse1-fan.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/testmpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Add required imports\r\n",
							"import com.microsoft.spark.sqlanalytics\r\n",
							"from com.microsoft.spark.sqlanalytics.Constants import Constants\r\n",
							"from pyspark.sql.functions import col \r\n",
							"\r\n",
							"# Read from existing internal table\r\n",
							"# test-synapse1-fan.sql.azuresynapse.net\r\n",
							"# https://testsynapse12.dfs.core.windows.net\r\n",
							"dfToReadFromTable = (spark.read.option(Constants.SERVER, \"test-synapse1-fan.sql.azuresynapse.net\")\r\n",
							"    .option(Constants.TEMP_FOLDER, \"abfss://test@testsynapse12.dfs.core.windows.net/\")\r\n",
							"    # .option(Constants.USER, \"sqladminuser\")\r\n",
							"    # .option(Constants.PASSWORD, \"Pa$$w0rd1234\")\r\n",
							"    .synapsesql(\"SynapseDedicatedSQLPool.dbo.Persons\")\r\n",
							"    .select(\"*\")\r\n",
							"    .limit(3)) \r\n",
							"\r\n",
							"# Show contents of the dataframe\r\n",
							"dfToReadFromTable.show()\r\n",
							"\r\n",
							"print(dfToReadFromTable)\r\n",
							"print(\"==================\")\r\n",
							"dfToReadFromTable.printSchema()\r\n",
							"\r\n",
							"\r\n",
							"print(dfToReadFromTable.limit(10))"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/exp_yoyaku_jisseki_renkei_cle')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9e6ac068-16db-4ad3-aaa8-650b5ca52e20"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import pyspark.sql.functions as F\r\n",
							"import datetime\r\n",
							"from azure.storage.blob import BlobServiceClient "
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run modules/data_cleansing_common"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Key Vault\r\n",
							"hash_parameter = get_secret_value(key_valut_link_service_name,key_container_name,param_solt1_name,param_iterations1_name,param_solt2_name,param_iterations2_name)\r\n",
							"hash_code = F.udf(lambda c: hash_value(c, hash_parameter), StringType())"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#ETL業務日付を取得処理\r\n",
							"def select_gyomu_dt():\r\n",
							"    try:\r\n",
							"        # ログ出力共通関数を実行し、処理開始ログを出力する。\r\n",
							"        log('I0001', 'select_gyomu_dt')\r\n",
							"\r\n",
							"        #テーブル読み込み共通関数を実行し、結果を変数「etl_gyomu_dt_data」に設定する。\r\n",
							"        etl_gyomu_dt_data = load_table(dl_link_service_name, 'ETL_GYOMU_DT')\r\n",
							"        \r\n",
							"        #カラム選択\r\n",
							"        etl_gyomu_data = etl_gyomu_dt_data.select(col('GYOMU_DT')).collect()[0]['GYOMU_DT']\r\n",
							"\r\n",
							"        #ログ出力共通関数を実行し、処理終了ログを出力する。\r\n",
							"        log('I0002', 'select_gyomu_dt')\r\n",
							"\r\n",
							"        #処理終了\r\n",
							"        return etl_gyomu_data\r\n",
							"    except Exception as e:\r\n",
							"        log('E0001', 'select_gyomu_dt', type(e), e)\r\n",
							"        raise e"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#対象ファイル取得処理\r\n",
							"def load_target_file(etl_dt):\r\n",
							"    try:\r\n",
							"        log('I0001', 'load_target_file')\r\n",
							"        #読み込み対象ファイルのスキーマを定義し、変数「schema_columns」に設定する。\r\n",
							"        # スキーマ項目\r\n",
							"        schema_columns = [\r\n",
							"            # 共通会員ID\r\n",
							"            \"KAIIN_ID\",\r\n",
							"            # 共通会員ID連番\r\n",
							"            \"KAIIN_ID_NO\",\r\n",
							"            # 操作日\r\n",
							"            \"SOSA_YMD\",\r\n",
							"            # 操作時刻\r\n",
							"            \"SOSA_TIME\",\r\n",
							"            # 操作内容\r\n",
							"            \"SOSA_NAIYO\",\r\n",
							"            # 未発券\r\n",
							"            \"MIHAKKEN\",\r\n",
							"            # 会員種別\r\n",
							"            \"KAIIN_SHUBETSU_CD\",\r\n",
							"            # 会員番号\r\n",
							"            \"KAIIN_NO\",\r\n",
							"            # 会員氏名\r\n",
							"            \"KAIIN_NAME\",\r\n",
							"            # お客様コード\r\n",
							"            \"KOKYAKU_CD\",\r\n",
							"            # 出張番号\r\n",
							"            \"SHUCCHO_NO\",\r\n",
							"            # 法人社員番号\r\n",
							"            \"HOJIN_SHAIN_NO\",\r\n",
							"            # 受付箇所\r\n",
							"            \"UKETSUKE_KASHO\",\r\n",
							"            # 端末識別ID\r\n",
							"            \"TANMATSU_SHIKIBETSU_ID\",\r\n",
							"            # 端末番号\r\n",
							"            \"TANMATSU_NO\",\r\n",
							"            # 統一端末番号\r\n",
							"            \"TOUITSU_TANMATSU_NO\",\r\n",
							"            # お預かり番号\r\n",
							"            \"AZUKARI_NO\",\r\n",
							"            # 乗車日\r\n",
							"            \"JOSHA_YMD\",\r\n",
							"            # 乗車駅\r\n",
							"            \"JOSHA_EKI_CD\",\r\n",
							"            # 降車駅\r\n",
							"            \"KOSHA_EKI_CD\",\r\n",
							"            # 設備\r\n",
							"            \"SETSUBI_CD\",\r\n",
							"            # 列車１\r\n",
							"            \"RESSHA_1_CD\",\r\n",
							"            # 列車番号１\r\n",
							"            \"RESSHA_1_NO\",\r\n",
							"            # 乗車駅１\r\n",
							"            \"JOSHA_EKI_1_CD\",\r\n",
							"            # 発時刻１\r\n",
							"            \"HATSU_1_TIME\",\r\n",
							"            # 降車駅１\r\n",
							"            \"KOSHA_EKI_1_CD\",\r\n",
							"            # 着時刻１\r\n",
							"            \"CHAKU_1_TIME\",\r\n",
							"            # 設備１\r\n",
							"            \"SETSUBI_1_CD\",\r\n",
							"            # 喫煙禁煙区分１\r\n",
							"            \"KITSUEN_KINEN_1_KBN\",\r\n",
							"            # 設備付帯コード１\r\n",
							"            \"SETSUBI_FUTAI_1_CD\",\r\n",
							"            # レールスターフラグ１\r\n",
							"            \"RERU_STAR_1_FLG\",\r\n",
							"            # 列車２\r\n",
							"            \"RESSHA_2_CD\",\r\n",
							"            # 列車番号２\r\n",
							"            \"RESSHA_2_NO\",\r\n",
							"            # 乗車駅２\r\n",
							"            \"JOSHA_EKI_2_CD\",\r\n",
							"            # 発時刻２\r\n",
							"            \"HATSU_2_TIME\",\r\n",
							"            # 降車駅２\r\n",
							"            \"KOSHA_EKI_2_CD\",\r\n",
							"            # 着時刻２\r\n",
							"            \"CHAKU_2_TIME\",\r\n",
							"            # 設備２\r\n",
							"            \"SETSUBI_2_CD\",\r\n",
							"            # 喫煙禁煙区分２\r\n",
							"            \"KITSUEN_KINEN_2_KBN\",\r\n",
							"            # 設備付帯コード２\r\n",
							"            \"SETSUBI_FUTAI_2_CD\",\r\n",
							"            # レールスターフラグ２\r\n",
							"            \"RERU_STAR_2_FLG\",\r\n",
							"            # 列車３\r\n",
							"            \"RESSHA_3_CD\",\r\n",
							"            # 列車番号３\r\n",
							"            \"RESSHA_3_NO\",\r\n",
							"            # 乗車駅３\r\n",
							"            \"JOSHA_EKI_3_CD\",\r\n",
							"            # 発時刻３\r\n",
							"            \"HATSU_3_TIME\",\r\n",
							"            # 降車駅３\r\n",
							"            \"KOSHA_EKI_3_CD\",\r\n",
							"            # 着時刻３\r\n",
							"            \"CHAKU_3_TIME\",\r\n",
							"            # 設備３\r\n",
							"            \"SETSUBI_3_CD\",\r\n",
							"            # 喫煙禁煙区分３\r\n",
							"            \"KITSUEN_KINEN_3_KBN\",\r\n",
							"            # 設備付帯コード３\r\n",
							"            \"SETSUBI_FUTAI_3_CD\",\r\n",
							"            # レールスターフラグ３\r\n",
							"            \"RERU_STAR_3_FLG\",\r\n",
							"            # きっぷの種類\r\n",
							"            \"KIPPU_SHURUI_CD\",\r\n",
							"            # 回数券名称（回数券お預かり番号）\r\n",
							"            \"KAISUKEN_NAME_CD\",\r\n",
							"            # 大人\r\n",
							"            \"OTONA_NIN\",\r\n",
							"            # 小児\r\n",
							"            \"SHONI_NIN\",\r\n",
							"            # 請求ベース\r\n",
							"            \"SEIKYU_BASE_MNY\",\r\n",
							"            # 発生ベース\r\n",
							"            \"HASSEI_BASE_MNY\",\r\n",
							"            # 請求（受取）\r\n",
							"            \"SEIKYU_UKETORI_MNY\",\r\n",
							"            # 発生（受取）\r\n",
							"            \"HASSEI_UKETORI_MNY\",\r\n",
							"            # 還元額\r\n",
							"            \"KANGENGAKU_MNY\",\r\n",
							"            # 還元内容\r\n",
							"            \"KANGEN_NAIYO\",\r\n",
							"            # ＡＧＴ会員ＩＤ\r\n",
							"            \"AGENT_KAIIN_ID\",\r\n",
							"            # 基本会員ＩＤ\r\n",
							"            \"KIHON_KAIIN_ID\",\r\n",
							"            # 親会員ＩＤ\r\n",
							"            \"OYA_KAIIN_ID\",\r\n",
							"            # 会員ＩＤ\r\n",
							"            \"KAIIN_2_ID\",\r\n",
							"            # ＡＧＴ会員番号\r\n",
							"            \"AGENT_KAIIN_NO\",\r\n",
							"            # 基本会員番号\r\n",
							"            \"KIHON_KAIIN_NO\",\r\n",
							"            # 親会員番号\r\n",
							"            \"OYA_KAIIN_NO\",\r\n",
							"            # 会員番号\r\n",
							"            \"KAIIN_2_NO\",\r\n",
							"        ]\r\n",
							"\r\n",
							"        #BLOBファイル読み込み共通関数を実行し、結果を変数「file_data」に設定する。\r\n",
							"        etl_dt_str = datetime.datetime.strftime(etl_dt,'%Y%m%d')\r\n",
							"        file_path = dwh_blob_folder_path\r\n",
							"        \r\n",
							"        file_list = load_dwh_blob_file_list(dwh_blob_link_service_name,dwh_blob_container_name,file_path)\r\n",
							"\r\n",
							"        schema_str = \",\".join(f\"{item} string\" for item in schema_columns)\r\n",
							"        target_df = spark.createDataFrame(spark.sparkContext.emptyRDD(),schema_str)\r\n",
							"\r\n",
							"        # 取込対象ファイル件数カウント用\r\n",
							"        file_cnt = 0\r\n",
							"\r\n",
							"        for file_name in file_list:\r\n",
							"            if str(file_name).__contains__('EXP_YOYAKU_JISSEKI_' + etl_dt_str):\r\n",
							"                file_data = load_file(dwh_blob_link_service_name, dwh_blob_container_name, file_path + file_name, schema_columns,'CP932',has_header = None)\r\n",
							"                target_df = target_df.unionByName(file_data)\r\n",
							"\r\n",
							"                # 取込対象ファイル数のカウント\r\n",
							"                file_cnt += 1\r\n",
							"\r\n",
							"                if file_data.count() == 0:\r\n",
							"                    log('W0001','load_target_file','No Data In File')\r\n",
							"        \r\n",
							"        # 取込対象ファイルが０件の場合は、取込対象ファイル無しとして異常終了させる\r\n",
							"        if file_cnt == 0:\r\n",
							"            raise FileNotFoundError(\"File Not Found\")\r\n",
							"\r\n",
							"        target_df = target_df \\\r\n",
							"        .withColumn(\"T_CRM_SHORI_YMD\", F.lit(etl_dt_str)) \\\r\n",
							"        .withColumn(\"T_CRM_UPDATE_DT\", F.current_timestamp()) \\\r\n",
							"        .withColumn(\"T_CRM_UPDATE_ID\", F.lit('CRM_ETL_01_02_01'))\r\n",
							"\r\n",
							"        #ログ出力共通関数を実行し、処理終了ログを出力する。\r\n",
							"        log('I0002', 'load_target_file')\r\n",
							"        return target_df\r\n",
							"    except Exception as e:\r\n",
							"        log('E0001', 'load_target_file', type(e), e)\r\n",
							"        raise e"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#対象データ整形処理\r\n",
							"def cleansing_data(target_df):\r\n",
							"    try:\r\n",
							"        log('I0001', 'cleansing_data')\r\n",
							"        \r\n",
							"        target_df = target_df.select(    \r\n",
							"            # 共通会員ID\r\n",
							"            hash_code(\"KAIIN_ID\").alias(\"KAIIN_ID\"),\r\n",
							"            # 共通会員ID連番\r\n",
							"            \"KAIIN_ID_NO\",\r\n",
							"            # 操作日\r\n",
							"            \"SOSA_YMD\",\r\n",
							"            # 操作時刻\r\n",
							"            \"SOSA_TIME\",\r\n",
							"            # 操作内容\r\n",
							"            \"SOSA_NAIYO\",\r\n",
							"            # 未発券\r\n",
							"            \"MIHAKKEN\",\r\n",
							"            # 会員種別\r\n",
							"            \"KAIIN_SHUBETSU_CD\",\r\n",
							"            # 会員番号\r\n",
							"            \"KAIIN_NO\",\r\n",
							"            # 会員氏名\r\n",
							"            \"KAIIN_NAME\",\r\n",
							"            # お客様コード\r\n",
							"            \"KOKYAKU_CD\",\r\n",
							"            # 出張番号\r\n",
							"            \"SHUCCHO_NO\",\r\n",
							"            # 法人社員番号\r\n",
							"            \"HOJIN_SHAIN_NO\",\r\n",
							"            # 受付箇所\r\n",
							"            \"UKETSUKE_KASHO\",\r\n",
							"            # 端末識別ID\r\n",
							"            \"TANMATSU_SHIKIBETSU_ID\",\r\n",
							"            # 端末番号\r\n",
							"            \"TANMATSU_NO\",\r\n",
							"            # 統一端末番号\r\n",
							"            \"TOUITSU_TANMATSU_NO\",\r\n",
							"            # お預かり番号\r\n",
							"            \"AZUKARI_NO\",\r\n",
							"            # 乗車日\r\n",
							"            \"JOSHA_YMD\",\r\n",
							"            # 乗車駅\r\n",
							"            \"JOSHA_EKI_CD\",\r\n",
							"            # 降車駅\r\n",
							"            \"KOSHA_EKI_CD\",\r\n",
							"            # 設備\r\n",
							"            \"SETSUBI_CD\",\r\n",
							"            # 列車１\r\n",
							"            \"RESSHA_1_CD\",\r\n",
							"            # 列車番号１\r\n",
							"            \"RESSHA_1_NO\",\r\n",
							"            # 乗車駅１\r\n",
							"            \"JOSHA_EKI_1_CD\",\r\n",
							"            # 発時刻１\r\n",
							"            \"HATSU_1_TIME\",\r\n",
							"            # 降車駅１\r\n",
							"            \"KOSHA_EKI_1_CD\",\r\n",
							"            # 着時刻１\r\n",
							"            \"CHAKU_1_TIME\",\r\n",
							"            # 設備１\r\n",
							"            \"SETSUBI_1_CD\",\r\n",
							"            # 喫煙禁煙区分１\r\n",
							"            \"KITSUEN_KINEN_1_KBN\",\r\n",
							"            # 設備付帯コード１\r\n",
							"            \"SETSUBI_FUTAI_1_CD\",\r\n",
							"            # レールスターフラグ１\r\n",
							"            \"RERU_STAR_1_FLG\",\r\n",
							"            # 列車２\r\n",
							"            \"RESSHA_2_CD\",\r\n",
							"            # 列車番号２\r\n",
							"            \"RESSHA_2_NO\",\r\n",
							"            # 乗車駅２\r\n",
							"            \"JOSHA_EKI_2_CD\",\r\n",
							"            # 発時刻２\r\n",
							"            \"HATSU_2_TIME\",\r\n",
							"            # 降車駅２\r\n",
							"            \"KOSHA_EKI_2_CD\",\r\n",
							"            # 着時刻２\r\n",
							"            \"CHAKU_2_TIME\",\r\n",
							"            # 設備２\r\n",
							"            \"SETSUBI_2_CD\",\r\n",
							"            # 喫煙禁煙区分２\r\n",
							"            \"KITSUEN_KINEN_2_KBN\",\r\n",
							"            # 設備付帯コード２\r\n",
							"            \"SETSUBI_FUTAI_2_CD\",\r\n",
							"            # レールスターフラグ２\r\n",
							"            \"RERU_STAR_2_FLG\",\r\n",
							"            # 列車３\r\n",
							"            \"RESSHA_3_CD\",\r\n",
							"            # 列車番号３\r\n",
							"            \"RESSHA_3_NO\",\r\n",
							"            # 乗車駅３\r\n",
							"            \"JOSHA_EKI_3_CD\",\r\n",
							"            # 発時刻３\r\n",
							"            \"HATSU_3_TIME\",\r\n",
							"            # 降車駅３\r\n",
							"            \"KOSHA_EKI_3_CD\",\r\n",
							"            # 着時刻３\r\n",
							"            \"CHAKU_3_TIME\",\r\n",
							"            # 設備３\r\n",
							"            \"SETSUBI_3_CD\",\r\n",
							"            # 喫煙禁煙区分３\r\n",
							"            \"KITSUEN_KINEN_3_KBN\",\r\n",
							"            # 設備付帯コード３\r\n",
							"            \"SETSUBI_FUTAI_3_CD\",\r\n",
							"            # レールスターフラグ３\r\n",
							"            \"RERU_STAR_3_FLG\",\r\n",
							"            # きっぷの種類\r\n",
							"            \"KIPPU_SHURUI_CD\",\r\n",
							"            # 回数券名称（回数券お預かり番号）\r\n",
							"            \"KAISUKEN_NAME_CD\",\r\n",
							"            # 大人\r\n",
							"            F.col(\"OTONA_NIN\").cast(DecimalType(2,0)).alias(\"OTONA_NIN\"),\r\n",
							"            # 小児\r\n",
							"            F.col(\"SHONI_NIN\").cast(DecimalType(2,0)).alias(\"SHONI_NIN\"),\r\n",
							"            # 請求ベース\r\n",
							"            F.col(\"SEIKYU_BASE_MNY\").cast(DecimalType(9,0)).alias(\"SEIKYU_BASE_MNY\"),\r\n",
							"            # 発生ベース\r\n",
							"            F.col(\"HASSEI_BASE_MNY\").cast(DecimalType(9,0)).alias(\"HASSEI_BASE_MNY\"),\r\n",
							"            # 請求（受取）\r\n",
							"            F.col(\"SEIKYU_UKETORI_MNY\").cast(DecimalType(9,0)).alias(\"SEIKYU_UKETORI_MNY\"),\r\n",
							"            # 発生（受取）\r\n",
							"            F.col(\"HASSEI_UKETORI_MNY\").cast(DecimalType(9,0)).alias(\"HASSEI_UKETORI_MNY\"),\r\n",
							"            # 還元額\r\n",
							"            F.col(\"KANGENGAKU_MNY\").cast(DecimalType(9,0)).alias(\"KANGENGAKU_MNY\"),\r\n",
							"            # 還元内容\r\n",
							"            \"KANGEN_NAIYO\",\r\n",
							"            # ＡＧＴ会員ＩＤ\r\n",
							"            \"AGENT_KAIIN_ID\",\r\n",
							"            # 基本会員ＩＤ\r\n",
							"            \"KIHON_KAIIN_ID\",\r\n",
							"            # 親会員ＩＤ\r\n",
							"            mask_camp_name(\"OYA_KAIIN_ID\").alias(\"OYA_KAIIN_ID\"),\r\n",
							"            # 会員ＩＤ\r\n",
							"            mask_camp_name(\"KAIIN_2_ID\").alias(\"KAIIN_2_ID\"),\r\n",
							"            # ＡＧＴ会員番号\r\n",
							"            \"AGENT_KAIIN_NO\",\r\n",
							"            # 基本会員番号\r\n",
							"            \"KIHON_KAIIN_NO\",\r\n",
							"            # 親会員番号\r\n",
							"            \"OYA_KAIIN_NO\",\r\n",
							"            # 会員番号\r\n",
							"            \"KAIIN_2_NO\",\r\n",
							"            # 統合CRM日次処理日\r\n",
							"            \"T_CRM_SHORI_YMD\",\r\n",
							"            # 統合CRM更新日時\r\n",
							"            F.to_timestamp(\"T_CRM_UPDATE_DT\", \"yyyy/MM/dd HH:mm:ss\").alias(\"T_CRM_UPDATE_DT\"),\r\n",
							"            # 統合CRM更新ID\r\n",
							"            \"T_CRM_UPDATE_ID\"\r\n",
							"        ).withColumn(\"SOSA_TIME\", F.lpad(\"SOSA_TIME\", 5, \"0\")) \\\r\n",
							"            .withColumn(\"HATSU_1_TIME\", F.lpad(\"HATSU_1_TIME\", 5, \"0\")) \\\r\n",
							"            .withColumn(\"CHAKU_1_TIME\", F.lpad(\"CHAKU_1_TIME\", 5, \"0\")) \\\r\n",
							"            .withColumn(\"HATSU_2_TIME\", F.lpad(\"HATSU_2_TIME\", 5, \"0\")) \\\r\n",
							"            .withColumn(\"CHAKU_2_TIME\", F.lpad(\"CHAKU_2_TIME\", 5, \"0\")) \\\r\n",
							"            .withColumn(\"HATSU_3_TIME\", F.lpad(\"HATSU_3_TIME\", 5, \"0\")) \\\r\n",
							"            .withColumn(\"CHAKU_3_TIME\", F.lpad(\"CHAKU_3_TIME\", 5, \"0\"))\r\n",
							"\r\n",
							"        #ログ出力共通関数を実行し、処理終了ログを出力する。\r\n",
							"        log('I0002', 'cleansing_data')\r\n",
							"        return target_df\r\n",
							"    except Exception as e:\r\n",
							"        log('E0001', 'cleansing_data', type(e), e)\r\n",
							"        raise e"
						],
						"outputs": []
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# メイン処理\r\n",
							"if __name__ == '__main__':\r\n",
							"    try:\r\n",
							"        #ログ出力共通関数を実行し、処理開始ログを出力する。\r\n",
							"        log('I0001', 'main')\r\n",
							"        \r\n",
							"        #ETL業務日付を取得処理実行\r\n",
							"        etl_dt = select_gyomu_dt()\r\n",
							"\r\n",
							"        #対象ファイル取得処理実行\r\n",
							"        target_df = load_target_file(etl_dt)\r\n",
							"\r\n",
							"        #対象データ整形処理実行\r\n",
							"        target_df = cleansing_data(target_df)\r\n",
							"        \r\n",
							"        #DB更新\r\n",
							"        #PKリスト\r\n",
							"        pk_list = ['KAIIN_ID','SOSA_YMD','SOSA_TIME','SOSA_NAIYO','AZUKARI_NO']\r\n",
							"        #重複排除項目名\r\n",
							"        duplicated_exclude_columns = [{\"col\": \"T_CRM_UPDATE_DT\", \"sort\": \"desc\"}]\r\n",
							"        \r\n",
							"        #差分更新\r\n",
							"        save_table_upsert(dl_link_service_name,'EXP_YOYAKU_JISSEKI',target_df,pk_list,duplicated_exclude_columns)\r\n",
							"        \r\n",
							"        #元ファイル削除\r\n",
							"        etl_dt_str = datetime.datetime.strftime(etl_dt,'%Y%m%d')\r\n",
							"        file_path = dwh_blob_folder_path\r\n",
							"        file_list = load_dwh_blob_file_list(dwh_blob_link_service_name,dwh_blob_container_name,file_path)\r\n",
							"        for file_name in file_list:\r\n",
							"            if str(file_name).__contains__('EXP_YOYAKU_JISSEKI_' + etl_dt_str):\r\n",
							"                delete_dwh_blob(dwh_blob_link_service_name,dwh_blob_container_name,file_path + file_name)\r\n",
							"        #処理終了        \r\n",
							"        log('I0002', 'メイン処理')\r\n",
							"    except Exception as e:\r\n",
							"        #エラー処理\r\n",
							"        log('E0001', 'main', type(e), e)\r\n",
							"        raise e"
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/matlib')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "testconf",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7b952884-c022-4f25-bfa4-3f0dd89418c1"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2/resourceGroups/test-synapse1/providers/Microsoft.Synapse/workspaces/test-synapse1-fan/bigDataPools/testconf",
						"name": "testconf",
						"type": "Spark",
						"endpoint": "https://test-synapse1-fan.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/testconf",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"print(\"hello\")"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import matplotlib.pyplot as plt\r\n",
							"import numpy as np\r\n",
							" \r\n",
							"# 二次曲線の作成\r\n",
							"x = np.linspace(-3,3)\r\n",
							"y = x**2\r\n",
							"  \r\n",
							"# 二次曲線のプロット作成\r\n",
							"plt.plot(x, y, label=\"二次曲線\")\r\n",
							" \r\n",
							"# タイトル・軸ラベル表示\r\n",
							"plt.title(\"グラフタイトル\")\r\n",
							"plt.xlabel(\"x軸ラベル名\")\r\n",
							"plt.ylabel(\"y軸ラベル名\")\r\n",
							" \r\n",
							"# グラフ内テキスト表示\r\n",
							"plt.text(0, 4,\"テキスト例\")\r\n",
							" \r\n",
							"# 凡例表示\r\n",
							"plt.legend()\r\n",
							" \r\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install pandas"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"pip install japanize-matplotlib"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import matplotlib.pyplot as plt\r\n",
							"import numpy as np\r\n",
							"import japanize_matplotlib\r\n",
							" \r\n",
							"# 二次曲線の作成\r\n",
							"x = np.linspace(-3,3)\r\n",
							"y = x**2\r\n",
							"  \r\n",
							"# 二次曲線のプロット作成\r\n",
							"plt.plot(x, y, label=\"二次曲線\")\r\n",
							" \r\n",
							"# タイトル・軸ラベル表示\r\n",
							"plt.title(\"グラフタイトル\")\r\n",
							"plt.xlabel(\"x軸ラベル名\")\r\n",
							"plt.ylabel(\"y軸ラベル名\")\r\n",
							" \r\n",
							"# グラフ内テキスト表示\r\n",
							"plt.text(0, 4,\"テキスト例\")\r\n",
							" \r\n",
							"# 凡例表示\r\n",
							"plt.legend()\r\n",
							" \r\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/mei_cle')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "47df7a4f-b358-4b08-b81f-6e2fca6be1eb"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"import pyspark.sql.functions as F\r\n",
							"import datetime\r\n",
							"from azure.storage.blob import BlobServiceClient"
						],
						"outputs": [],
						"execution_count": 57
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run modules/data_cleansing_common"
						],
						"outputs": [],
						"execution_count": 58
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#ETL業務日付を取得処理\r\n",
							"def select_gyomu_dt():\r\n",
							"    try:\r\n",
							"        # ログ出力共通関数を実行し、処理開始ログを出力する。\r\n",
							"        log('I0001', 'select_gyomu_dt')\r\n",
							"\r\n",
							"        #テーブル読み込み共通関数を実行し、結果を変数「etl_gyomu_dt_data」に設定する。\r\n",
							"        etl_gyomu_dt_data = load_table(pi_link_service_name, 'ETL_GYOMU_DT')\r\n",
							"        \r\n",
							"        #カラム選択\r\n",
							"        etl_gyomu_data = etl_gyomu_dt_data.select(col('GYOMU_DT')).collect()[0]['GYOMU_DT']\r\n",
							"\r\n",
							"        #ログ出力共通関数を実行し、処理終了ログを出力する。\r\n",
							"        log('I0002', 'select_gyomu_dt')\r\n",
							"\r\n",
							"        #処理終了\r\n",
							"        return etl_gyomu_data\r\n",
							"    except Exception as e:\r\n",
							"        log('E0001', 'select_gyomu_dt', type(e), e)\r\n",
							"        raise e"
						],
						"outputs": [],
						"execution_count": 62
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#対象ファイル取得処理\r\n",
							"def load_target_file(etl_dt):\r\n",
							"    try:\r\n",
							"        log('I0001', 'load_target_file')\r\n",
							"        #読み込み対象ファイルのスキーマを定義し、変数「schema_columns」に設定する。\r\n",
							"        # スキーマ項目\r\n",
							"        schema_columns = StructType([\r\n",
							"            StructField('IDI', StringType(), False),\r\n",
							"            StructField('RCYCL_ID', StringType(), False),\r\n",
							"            StructField('SET_BTMSK', StringType(), True),\r\n",
							"            StructField('MEI_ID', StringType(), False),\r\n",
							"            StructField('PROC_CD', StringType(), True),\r\n",
							"            StructField('EKIMU_KIKI_BP_CD', StringType(), True),\r\n",
							"            StructField('KISHU_CD', StringType(), True),\r\n",
							"            StructField('EKIMU_KIKI_ID', StringType(), True),\r\n",
							"            StructField('EKIMU_KIKI_START4', StringType(), True),\r\n",
							"            StructField('EKIMU_KIKI_END4', StringType(), True),\r\n",
							"            StructField('IC_NUM', StringType(), True),\r\n",
							"            StructField('YMD', StringType(), True),\r\n",
							"            StructField('TM', StringType(), True),\r\n",
							"            StructField('FNC_CLS', StringType(), True),\r\n",
							"            StructField('CARD_CTL_CD', StringType(), True),\r\n",
							"            StructField('USE_EKI1', StringType(), True),\r\n",
							"            StructField('USE_EKI2', StringType(), True),\r\n",
							"            StructField('PROC_CLS', StringType(), True),\r\n",
							"            StructField('USE_EKI_CLS', StringType(), True),\r\n",
							"            StructField('OTHER_EXAM_TICT_PSSG', StringType(), True),\r\n",
							"            StructField('OLD_CDIDI', StringType(), True),\r\n",
							"            StructField('OLD_CARD_RCYCL_ID', StringType(), True),\r\n",
							"            StructField('SUM_USE_GK', StringType(), True),\r\n",
							"            StructField('SF_LOG_ID', StringType(), True),\r\n",
							"            StructField('BP_CD_SF1', StringType(), True),\r\n",
							"            StructField('USE_GK_SF1', StringType(), True),\r\n",
							"            StructField('ZAN_GK_SF1', StringType(), True),\r\n",
							"            StructField('PAYM_KBN_SF1', StringType(), True),\r\n",
							"            StructField('PLN_SKBT_CD', StringType(), True),\r\n",
							"            StructField('SETT_BP_CD', StringType(), True),\r\n",
							"            StructField('DEPO_GK', StringType(), True),\r\n",
							"            StructField('PAYM_KBN_DEPO', StringType(), True),\r\n",
							"            StructField('CRSPND_MIRYO_FLG', StringType(), True),\r\n",
							"            StructField('SF_DNM_CD', StringType(), True),\r\n",
							"            StructField('SAL_CTL_YMD', StringType(), True),\r\n",
							"            StructField('DISCNT1_CMPNY1', StringType(), True),\r\n",
							"            StructField('DISCNT1_CMPNY2', StringType(), True),\r\n",
							"            StructField('DISCNT2_CMPNY2', StringType(), True),\r\n",
							"            StructField('DISCNT2_CMPNY3', StringType(), True),\r\n",
							"            StructField('DISCNT3_CMPNY3', StringType(), True),\r\n",
							"            StructField('DISCNT3_CMPNY4', StringType(), True),\r\n",
							"            StructField('RN_EKI1', StringType(), True),\r\n",
							"            StructField('RN_EKI2', StringType(), True),\r\n",
							"            StructField('RN_EKI3', StringType(), True),\r\n",
							"            StructField('ENTRY_EXIT', StringType(), True),\r\n",
							"            StructField('SEASON_USE', StringType(), True),\r\n",
							"            StructField('SF_USE', StringType(), True),\r\n",
							"            StructField('COMMON_RESERVE1', StringType(), True),\r\n",
							"            StructField('SALE_3x', StringType(), True),\r\n",
							"            StructField('SALE_2x', StringType(), True),\r\n",
							"            StructField('SALE_1x', StringType(), True),\r\n",
							"            StructField('SEASON_CALC', StringType(), True),\r\n",
							"            StructField('PASS_SERVICE', StringType(), True),\r\n",
							"            StructField('PRIOR_TRAIN_ENTRY_EXIT', StringType(), True),\r\n",
							"            StructField('COMMON_RESERVE2', StringType(), True),\r\n",
							"            StructField('COMMON_RESERVE3', StringType(), True),\r\n",
							"            StructField('BUS_TRAIN_USE', StringType(), True),\r\n",
							"            StructField('BUSTORW_RWTOBUS_SALE', StringType(), True),\r\n",
							"            StructField('COMMON_RESERVE4', StringType(), True),\r\n",
							"            StructField('TEMP_CALC_STATION_NUM', StringType(), True),\r\n",
							"            StructField('JOSHA_STR_EKI', StringType(), True),\r\n",
							"            StructField('JISEN_INS_PEAK_FARE', StringType(), True),\r\n",
							"            StructField('USE_EKI1_AREA_SKBT_CD', StringType(), True),\r\n",
							"            StructField('USE_EKI2_AREA_SKBT_CD', StringType(), True),\r\n",
							"            StructField('OPRT_DATE_ID_BIT', StringType(), True),\r\n",
							"            StructField('PROC_YMD', StringType(), True),\r\n",
							"            StructField('NUM', StringType(), True),\r\n",
							"            StructField('PROC_YMD_IDX', StringType(), True),\r\n",
							"        ])\r\n",
							"\r\n",
							"        #BLOBファイル読み込み共通関数を実行し、結果を変数「target_df」に設定する。\r\n",
							"        etl_dt_str = datetime.datetime.strftime(etl_dt,'%Y%m%d')\r\n",
							"        \r\n",
							"        #特別処理:複数ファイル連携\r\n",
							"        file_path = blob_folder_path\r\n",
							"        file_list = load_adls_blob_file_list(blob_link_service_name,blob_container_name,file_path)\r\n",
							"        file_list.sort()\r\n",
							"        target_df = spark.createDataFrame(spark.sparkContext.emptyRDD(),schema_columns)\r\n",
							"\r\n",
							"        # 取込対象ファイル件数カウント用\r\n",
							"        file_cnt = 0\r\n",
							"\r\n",
							"        for file_name in file_list:\r\n",
							"            if str(file_name).__contains__('mei_' + etl_dt_str + '_'):\r\n",
							"\r\n",
							"                # 取込対象ファイル数のカウント\r\n",
							"                file_cnt += 1\r\n",
							"\r\n",
							"                file_name_path = file_path + file_name\r\n",
							"                file_data = load_adls_file(blob_link_service_name, blob_container_name, file_name_path, schema_columns,'csv',True)\r\n",
							"                target_df = target_df.unionByName(file_data)\r\n",
							"                if file_data.count() == 0:\r\n",
							"                    log('W0001','load_target_file','No Data In File')\r\n",
							"\r\n",
							"        # 取込対象ファイルが０件の場合は、取込対象ファイル無しとして異常終了させる\r\n",
							"        if file_cnt == 0:\r\n",
							"            raise FileNotFoundError(\"File Not Found\")\r\n",
							"\r\n",
							"        target_df = target_df.withColumn(\"T_CRM_SHORI_YMD\", F.lit(etl_dt_str)) \\\r\n",
							"            .withColumn(\"T_CRM_UPDATE_DT\", F.current_timestamp()) \\\r\n",
							"            .withColumn(\"T_CRM_UPDATE_ID\", F.lit('CRM_ETL_02_01_01'))\r\n",
							"\r\n",
							"        #ログ出力共通関数を実行し、処理終了ログを出力する。\r\n",
							"        log('I0002', 'load_target_file')\r\n",
							"        return target_df\r\n",
							"    except Exception as e:\r\n",
							"        log('E0001', 'load_target_file', type(e), e)\r\n",
							"        raise e"
						],
						"outputs": [],
						"execution_count": 63
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#対象データ整形処理\r\n",
							"def cleansing_data(target_df,etl_dt):\r\n",
							"    try:\r\n",
							"        log('I0001', 'cleansing_data')\r\n",
							"        etl_dt_str = datetime.datetime.strftime(etl_dt,'%Y%m%d')\r\n",
							"    \r\n",
							"        target_df = target_df.withColumn('RCYCL_ID',F.col('RCYCL_ID').cast(ShortType())) \\\r\n",
							"                            .withColumn('MEI_ID',F.col('MEI_ID').cast(IntegerType())) \\\r\n",
							"                            .withColumn('OLD_CARD_RCYCL_ID',F.col('OLD_CARD_RCYCL_ID').cast(ShortType())) \\\r\n",
							"                            .withColumn('SUM_USE_GK',F.col('SUM_USE_GK').cast(DecimalType(6,0))) \\\r\n",
							"                            .withColumn('SF_LOG_ID',F.col('SF_LOG_ID').cast(IntegerType())) \\\r\n",
							"                            .withColumn('USE_GK_SF1',F.col('USE_GK_SF1').cast(DecimalType(6,0))) \\\r\n",
							"                            .withColumn('ZAN_GK_SF1',F.col('ZAN_GK_SF1').cast(DecimalType(6,0))) \\\r\n",
							"                            .withColumn('DEPO_GK',F.col('DEPO_GK').cast(DecimalType(6,0))) \\\r\n",
							"                            .withColumn('DISCNT1_CMPNY1',F.col('DISCNT1_CMPNY1').cast(DecimalType(6,0))) \\\r\n",
							"                            .withColumn('DISCNT1_CMPNY2',F.col('DISCNT1_CMPNY2').cast(DecimalType(6,0))) \\\r\n",
							"                            .withColumn('DISCNT2_CMPNY2',F.col('DISCNT2_CMPNY2').cast(DecimalType(6,0))) \\\r\n",
							"                            .withColumn('DISCNT2_CMPNY3',F.col('DISCNT2_CMPNY3').cast(DecimalType(6,0))) \\\r\n",
							"                            .withColumn('DISCNT3_CMPNY3',F.col('DISCNT3_CMPNY3').cast(DecimalType(6,0))) \\\r\n",
							"                            .withColumn('DISCNT3_CMPNY4',F.col('DISCNT3_CMPNY4').cast(DecimalType(6,0))) \\\r\n",
							"                            .withColumn('JISEN_INS_PEAK_FARE',F.col('JISEN_INS_PEAK_FARE').cast(DecimalType(4,0))) \\\r\n",
							"                            .withColumn('NUM',F.col('NUM').cast(LongType())) \\\r\n",
							"\r\n",
							"        target_df = target_df \\\r\n",
							"            .withColumn(\"T_CRM_SHORI_YMD\", F.lit(etl_dt_str)) \\\r\n",
							"            .withColumn(\"T_CRM_UPDATE_DT\", F.current_timestamp()) \\\r\n",
							"            .withColumn(\"T_CRM_UPDATE_ID\", F.lit('CRM_ETL_02_01_01'))\r\n",
							"\r\n",
							"        #ログ出力共通関数を実行し、処理終了ログを出力する。\r\n",
							"        log('I0002', 'cleansing_data')\r\n",
							"        return target_df\r\n",
							"    except Exception as e:\r\n",
							"        log('E0001', 'cleansing_data', type(e), e)\r\n",
							"        raise e"
						],
						"outputs": [],
						"execution_count": 64
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# メイン処理\r\n",
							"if __name__ == '__main__':\r\n",
							"    try:\r\n",
							"        #ログ出力共通関数を実行し、処理開始ログを出力する。\r\n",
							"        log('I0001', 'main')\r\n",
							"        \r\n",
							"        #ETL業務日付を取得処理実行\r\n",
							"        etl_dt = select_gyomu_dt()\r\n",
							"\r\n",
							"        #対象ファイル取得処理実行\r\n",
							"        target_df = load_target_file(etl_dt)\r\n",
							"\r\n",
							"        #対象データ整形処理実行\r\n",
							"        target_df = cleansing_data(target_df,etl_dt)\r\n",
							"\r\n",
							"        #新規：ID系一件明細\r\n",
							"        save_table(pi_link_service_name,'ICC_ID_IKKEN_MEISAI',target_df)\r\n",
							"\r\n",
							"        # 元ファイル削除\r\n",
							"        file_path = blob_folder_path\r\n",
							"        file_list = load_adls_blob_file_list(blob_link_service_name,blob_container_name,file_path)\r\n",
							"\r\n",
							"        etl_dt_str = datetime.datetime.strftime(etl_dt,'%Y%m%d')\r\n",
							"        for file_name in file_list:\r\n",
							"\r\n",
							"            if str(file_name).__contains__('mei_' + etl_dt_str + '_'):\r\n",
							"                file_name_path = file_path + file_name\r\n",
							"                delete_adls_blob(blob_link_service_name, blob_container_name,file_name_path)\r\n",
							"\r\n",
							"        #処理終了        \r\n",
							"        log('I0002', 'メイン処理')\r\n",
							"    except Exception as e:\r\n",
							"        #エラー処理\r\n",
							"        log('E0001', 'main', type(e), e)\r\n",
							"        raise e"
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/testB')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "6595a2b6-d0e2-4898-b7c4-6bf9b111f9f5"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"print(\"tsetB\")"
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/testC')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "8622daa1-8340-4369-97d2-1dc6b85d55f9"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"import time\r\n",
							"\r\n",
							"time.sleep(5)\r\n",
							"\r\n",
							"print(\"TestC\")\r\n",
							"print(\"TestC\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(\"hello\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"ping www.google.com"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						]
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/testD')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "testDotNt",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7fcee4c1-c733-4e9b-b559-c97e34e9c6e4"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2/resourceGroups/test-synapse1/providers/Microsoft.Synapse/workspaces/test-synapse1-fan/bigDataPools/testDotNt",
						"name": "testDotNt",
						"type": "Spark",
						"endpoint": "https://test-synapse1-fan.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/testDotNt",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							}
						},
						"source": [
							"%%pyspark\r\n",
							"# Set the required configs\r\n",
							"source_full_storage_account_name = \"yikeren1020.dfs.core.windows.net\"\r\n",
							"spark.conf.set(f\"spark.storage.synapse.{source_full_storage_account_name}.linkedServiceName\", \"ADLSGen2Test2\")\r\n",
							"sc._jsc.hadoopConfiguration().set(f\"fs.azure.account.auth.type.{source_full_storage_account_name}\", \"SAS\")\r\n",
							"sc._jsc.hadoopConfiguration().set(f\"fs.azure.sas.token.provider.type.{source_full_storage_account_name}\", \"com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedSASProvider\")\r\n",
							"\r\n",
							"# Python code\r\n",
							"df = spark.read.csv('abfss://test1020@yikeren1020.dfs.core.windows.net/fan/test.csv')\r\n",
							"\r\n",
							"df.show()"
						],
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/testtalk')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "test"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "testDotNt",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "c4ce9e9c-ae8d-43bd-956e-177ea8ef944a"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/cc7a7151-8b95-49b7-aaf9-6802e1f1f6d2/resourceGroups/test-synapse1/providers/Microsoft.Synapse/workspaces/test-synapse1-fan/bigDataPools/testDotNt",
						"name": "testDotNt",
						"type": "Spark",
						"endpoint": "https://test-synapse1-fan.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/testDotNt",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.3",
						"nodeCount": 10,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"print(\"hello\")\r\n",
							"\r\n",
							"name = input('Enter your name: ')\r\n",
							"\r\n",
							"print('Hello, ' + name + '!')\r\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sparkConfiguration1')]",
			"type": "Microsoft.Synapse/workspaces/sparkConfigurations",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"configs": {
					"autoBroadcastJoinThreshold": "-1"
				},
				"created": "2023-03-15T01:33:05.173Z",
				"createdBy": "fanxin@microsoft.com",
				"annotations": [
					"test"
				],
				"configMergeRule": {
					"artifact.currentOperation.autoBroadcastJoinThreshold": "replace"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sparkConfiguration2')]",
			"type": "Microsoft.Synapse/workspaces/sparkConfigurations",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"configs": {
					"spark.synapse.logAnalytics.enabled": "true",
					"spark.synapse.logAnalytics.workspaceId": "64967e6d-04df-4cb2-ad93-c82c2b44ba6f",
					"spark.synapse.logAnalytics.secret": "YV8b94k8pwRWNAUve9x0gYaGvKJ/hCoNwwBrExi59R9bf5DOwGRqqH00xeNSQJUqod0l1wTwx5gRleC6jnLtXg=="
				},
				"created": "2023-10-30T14:29:27.109Z",
				"createdBy": "fanxin@microsoft.com",
				"annotations": [],
				"configMergeRule": {
					"artifact.currentOperation.spark.synapse.logAnalytics.enabled": "replace",
					"artifact.currentOperation.spark.synapse.logAnalytics.workspaceId": "replace",
					"artifact.currentOperation.spark.synapse.logAnalytics.secret": "replace"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SynapseDedicatedSQLPool')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "ukwest"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/testsparkpool')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 20,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.2",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "ukwest"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/testmpool')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.2",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "ukwest"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/testDotNt')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 20,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.3",
				"libraryRequirements": {
					"content": "azure-storage-file-datalake",
					"filename": "Requirements.txt",
					"time": "2023-02-17T06:22:42.8006799Z"
				},
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "ukwest"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/testconf')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.3",
				"isComputeIsolationEnabled": false,
				"sparkConfigProperties": {
					"configurationType": "Artifact",
					"filename": "sparkConfiguration2",
					"content": "{\"name\":\"sparkConfiguration2\",\"properties\":{\"configs\":{\"spark.synapse.logAnalytics.enabled\":\"true\",\"spark.synapse.logAnalytics.workspaceId\":\"64967e6d-04df-4cb2-ad93-c82c2b44ba6f\",\"spark.synapse.logAnalytics.secret\":\"YV8b94k8pwRWNAUve9x0gYaGvKJ/hCoNwwBrExi59R9bf5DOwGRqqH00xeNSQJUqod0l1wTwx5gRleC6jnLtXg==\"},\"annotations\":[],\"type\":\"Microsoft.Synapse/workspaces/sparkconfigurations\",\"description\":\"\",\"notes\":\"\",\"created\":\"2023-10-30T23:29:27.1090000+09:00\",\"createdBy\":\"fanxin@microsoft.com\",\"configMergeRule\":{\"admin.currentOperation.spark.synapse.logAnalytics.enabled\":\"replace\",\"admin.currentOperation.spark.synapse.logAnalytics.workspaceId\":\"replace\",\"admin.currentOperation.spark.synapse.logAnalytics.secret\":\"replace\"}}}",
					"time": "2023-10-30T14:31:12.5473627Z"
				},
				"sessionLevelPackagesEnabled": false,
				"annotations": []
			},
			"dependsOn": [],
			"location": "ukwest"
		}
	]
}